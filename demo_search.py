#!/usr/bin/env python3
"""
æ–‡çŒ®è¯­ä¹‰æœç´¢ - æˆªå›¾æ¼”ç¤ºè„šæœ¬
ç”¨æ³•: python3 demo_search.py "ä¸­æ–‡æŸ¥è¯¢" ["--also" "English query"]
åŠŸèƒ½: ç¾åŒ–è¾“å‡ºï¼Œé€‚åˆæˆªå›¾/æ¼”ç¤º
"""

import sys
import os
import time

# åœ¨å¯¼å…¥ä»»ä½•ä¼šäº§ç”Ÿè¿›åº¦æ¡çš„åº“ä¹‹å‰ï¼Œé‡å®šå‘ stderr ä»¥å±è”½ tqdm/transformers åŠ è½½ä¿¡æ¯
import io
_original_stderr = sys.stderr
sys.stderr = io.StringIO()

# é¢„åŠ è½½ä¼šäº§ç”Ÿå™ªå£°è¾“å‡ºçš„åº“
try:
    import warnings
    warnings.filterwarnings("ignore")
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TRANSFORMERS_VERBOSITY"] = "error"
    from sentence_transformers import SentenceTransformer
    import numpy as np
except ImportError:
    pass

# æ¢å¤ stderrï¼ˆrich éœ€è¦ç”¨åˆ°ï¼‰
sys.stderr = _original_stderr

# ===========================
import json
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from rich.rule import Rule
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich import box

# ========= è·¯å¾„é…ç½® =========
BASE = Path(__file__).parent
INDEX_PATH = BASE / "paper_index.json"
EMBEDDINGS_PATH = BASE / "paper_embeddings.npz"

console = Console(width=100, highlight=False)

# ========= åŠ è½½æ•°æ® =========
def load_index():
    with open(INDEX_PATH, encoding="utf-8") as f:
        data = json.load(f)
    papers = list(data["papers"].values()) if isinstance(data.get("papers"), dict) else data.get("papers", [])
    stats = data.get("stats", {})
    return papers, stats

def load_embeddings():
    if not EMBEDDINGS_PATH.exists():
        return None, None, None
    d = np.load(EMBEDDINGS_PATH, allow_pickle=True)
    return d["embeddings"], list(d["filenames"]), str(d.get("model_name", "unknown"))

# ========= è¯­ä¹‰æœç´¢ =========
def get_model(model_name):
    """é™é»˜åŠ è½½æ¨¡å‹"""
    stderr_capture = io.StringIO()
    old_stderr = sys.stderr
    sys.stderr = stderr_capture
    try:
        model = SentenceTransformer(model_name)
    finally:
        sys.stderr = old_stderr
    return model

def semantic_search(query, papers, embeddings, filenames, model, top_n=5):
    """è¿”å› (paper, score) åˆ—è¡¨"""
    q_emb = model.encode([query], convert_to_numpy=True, show_progress_bar=False)[0]
    q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-8)

    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8
    emb_norm = embeddings / norms
    sims = emb_norm @ q_norm

    # å»ºç«‹æ–‡ä»¶åâ†’paperæ˜ å°„
    fname_to_paper = {p["filename"]: p for p in papers}
    results = []
    top_idx = np.argsort(sims)[::-1][:top_n * 3]
    for idx in top_idx:
        fname = filenames[idx]
        p = fname_to_paper.get(fname)
        if p:
            results.append((p, float(sims[idx])))
        if len(results) >= top_n:
            break
    return results

# ========= ç¾åŒ–è¾“å‡º =========
def make_score_bar(score, width=12):
    """å°†ç›¸ä¼¼åº¦åˆ†æ•°è½¬æ¢ä¸ºå¯è§†åŒ–è¿›åº¦æ¡"""
    filled = int(score * width)
    bar = "â–ˆ" * filled + "â–‘" * (width - filled)
    return bar

def format_title(p):
    """æå–æœ€ä½³æ˜¾ç¤ºæ ‡é¢˜"""
    title = p.get("zotero_title") or p.get("title_extracted") or p["filename"]
    # å»æ‰æ–‡ä»¶æ‰©å±•å
    if title.endswith(".pdf"):
        title = title[:-4]
    # æˆªæ–­è¿‡é•¿æ ‡é¢˜
    if len(title) > 80:
        title = title[:77] + "..."
    return title

def format_authors(p):
    authors = p.get("zotero_authors", [])
    if not authors:
        return ""
    if len(authors) == 1:
        return authors[0]
    elif len(authors) <= 3:
        return ", ".join(authors)
    else:
        return f"{authors[0]} ç­‰ ({len(authors)}äºº)"

def print_demo_results(query, results, elapsed, stats):
    """ç¾åŒ–æ‰“å°æœç´¢ç»“æœ"""

    # ===== æ ‡é¢˜é¢æ¿ =====
    header_text = Text()
    header_text.append("ğŸ“š æœ¬åœ°æ–‡çŒ®è¯­ä¹‰æœç´¢ç³»ç»Ÿ\n", style="bold white")
    header_text.append(f"  ç´¢å¼•: {stats.get('total_papers', '?')} ç¯‡è®ºæ–‡  ", style="dim white")
    header_text.append(f"æœ¬åœ° {stats.get('local_papers', '?')} + Zotero {stats.get('zotero_papers', '?')}  ", style="dim cyan")
    header_text.append(f"ä¸­æ–‡ {stats.get('chinese_papers', '?')} / è‹±æ–‡ {stats.get('english_papers', '?')}", style="dim green")

    console.print(Panel(
        header_text,
        border_style="bright_blue",
        padding=(0, 2),
    ))

    # ===== æŸ¥è¯¢ä¿¡æ¯ =====
    console.print()
    console.print(f"  [bold yellow]ğŸ” æŸ¥è¯¢:[/]  [bold white]{query}[/]")
    console.print(f"  [bold cyan]âš¡ æœç´¢:[/]  [bold white]{elapsed:.2f} ç§’[/]  [dim](å‘é‡è®¡ç®— + ç›¸ä¼¼åº¦æ’åº)[/]")
    console.print(f"  [bold green]âœ“  æ‰¾åˆ°:[/]  [bold white]{len(results)} ç¯‡æœ€ç›¸å…³æ–‡çŒ®[/]")
    console.print()
    console.print(Rule("[dim]æœç´¢ç»“æœ[/]", style="bright_blue"))
    console.print()

    # ===== ç»“æœåˆ—è¡¨ =====
    for rank, (p, score) in enumerate(results, 1):
        # ç›¸å…³åº¦é¢œè‰²
        if score >= 0.7:
            score_color = "bright_green"
        elif score >= 0.5:
            score_color = "yellow"
        else:
            score_color = "white"

        title = format_title(p)
        authors = format_authors(p)
        year = p.get("year") or "å¹´ä»½æœªçŸ¥"
        lang = "ğŸ‡¨ğŸ‡³ ä¸­æ–‡" if p.get("language") == "zh" else "ğŸ‡ºğŸ‡¸ è‹±æ–‡"
        is_thesis = "ğŸ“ å­¦ä½è®ºæ–‡" if p.get("is_thesis") else ""
        source_tag = "[cyan]Z[/cyan]" if p.get("source") == "zotero" else "[green]L[/green]"
        bar = make_score_bar(score)

        # æ„å»ºæ¡ç›®
        rank_text = f"[bold white on bright_blue] {rank} [/]"
        console.print(
            f"  {rank_text}  [{score_color}]{bar}[/]  [{score_color}]{score:.3f}[/]  "
            f"{source_tag}"
        )
        console.print(f"     [bold white]{title}[/]")

        meta_parts = [f"[dim]{year}[/]", f"[dim]{lang}[/]"]
        if is_thesis:
            meta_parts.append(f"[dim yellow]{is_thesis}[/]")
        if authors:
            meta_parts.append(f"[dim]{authors}[/]")
        console.print("     " + "  |  ".join(meta_parts))

        # æ‘˜è¦ç‰‡æ®µ
        abstract = p.get("abstract", "")
        if abstract:
            snippet = abstract[:160].replace("\n", " ").strip()
            if len(abstract) > 160:
                snippet += "..."
            console.print(f"     [dim italic]{snippet}[/]")

        console.print()

    # ===== åº•éƒ¨æç¤º =====
    console.print(Rule(style="dim"))
    console.print(
        f"  [dim]ğŸ”§ ç³»ç»Ÿï¼š1244ç¯‡è®ºæ–‡ Â· å¤šè¯­è¨€è¯­ä¹‰å‘é‡ Â· ä¸­æ–‡æŸ¥è¯¢â†’è‹±æ–‡æ–‡çŒ®åŒ¹é…[/]"
    )
    console.print(
        f"  [dim]ğŸ“‚ æ¨¡å‹ï¼šparaphrase-multilingual-MiniLM-L12-v2 (384ç»´)[/]"
    )
    console.print()


def main():
    args = sys.argv[1:]
    if not args:
        console.print("[red]ç”¨æ³•: python3 demo_search.py \"æœç´¢æŸ¥è¯¢\" [--also \"é¢å¤–æŸ¥è¯¢\"][/]")
        console.print("[dim]ç¤ºä¾‹: python3 demo_search.py \"æ¤è¢«ç‰©å€™å¯¹è’¸æ•£å‘çš„å½±å“\" --also \"vegetation phenology evapotranspiration\"[/]")
        return

    # è§£æå‚æ•°
    query_parts = []
    also_parts = []
    i = 0
    while i < len(args):
        if args[i] == "--also" and i + 1 < len(args):
            also_parts.append(args[i + 1])
            i += 2
        elif args[i].startswith("--"):
            i += 1
        else:
            query_parts.append(args[i])
            i += 1

    query = " ".join(query_parts)
    also_queries = also_parts

    # åˆå¹¶æ‰€æœ‰æŸ¥è¯¢
    all_queries = [query] + also_queries
    full_query = " ".join(all_queries)

    # åŠ è½½æ•°æ®ï¼ˆé™é»˜ï¼‰
    papers, stats = load_index()
    embeddings, filenames, model_name = load_embeddings()

    if embeddings is None:
        console.print("[red]âš  æœªæ‰¾åˆ°å‘é‡ç´¢å¼•ï¼Œè¯·å…ˆè¿è¡Œ build_embeddings.py[/]")
        return

    # åŠ è½½æ¨¡å‹ï¼ˆä¸è®¡å…¥æœç´¢æ—¶é—´ï¼Œé™é»˜åŠ è½½ï¼‰
    model = get_model("paraphrase-multilingual-MiniLM-L12-v2")

    # æœç´¢ï¼ˆä»…è®¡å‘é‡ç¼–ç +ç›¸ä¼¼åº¦è®¡ç®—æ—¶é—´ï¼‰
    t0 = time.time()
    results = semantic_search(full_query, papers, embeddings, filenames, model, top_n=5)
    elapsed = time.time() - t0

    # æ‰“å°ç»“æœ
    display_query = query
    if also_queries:
        display_query += f" + {' '.join(also_queries)}"
    print_demo_results(display_query, results, elapsed, stats)


if __name__ == "__main__":
    main()
