#!/usr/bin/env python3
"""
æ–‡çŒ®ç´¢å¼•æ„å»ºå™¨ v3 - Zoteroé›†æˆ + jiebaåˆ†è¯
æ”¹è¿›ç‚¹(ç›¸æ¯”v2):
  1. æ‰«æZoteroå­˜å‚¨ç›®å½•ï¼ˆè·¨æ–‡ä»¶ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼‰
  2. ä»Zotero sqliteè¯»å–å…ƒæ•°æ®ï¼ˆåˆ†ç±»/æ ‡ç­¾/æ ‡é¢˜/ä½œè€…ï¼‰
  3. ä½¿ç”¨jiebaåˆ†è¯æ›¿ä»£n-gramï¼ˆå¤§å¹…æå‡æœç´¢å‡†ç¡®æ€§ï¼‰
  4. æŒ‰æ–‡ä»¶åå»é‡ï¼ˆåŒä¸€è®ºæ–‡åœ¨æœ¬åœ°å’ŒZoteroéƒ½æœ‰æ—¶åªä¿ç•™æœ¬åœ°ç‰ˆï¼‰

è¿è¡Œæ–¹å¼:
  python3 build_paper_index.py            # å…¨é‡é‡å»º
  python3 build_paper_index.py --incr     # å¢é‡æ›´æ–°ï¼ˆåªå¤„ç†æ–°æ–‡ä»¶ï¼‰
  python3 build_paper_index.py --no-zotero  # ä¸æ‰«æZotero
"""

import fitz  # PyMuPDF
import json
import os
import re
import sys
import time
import sqlite3
import jieba
from pathlib import Path
from collections import Counter, defaultdict

# å…³é—­MuPDFå†—ä½™è­¦å‘Šï¼Œé¿å…ä¸ªåˆ«å¼‚å¸¸æ³¨é‡ŠPDFåˆ·å±æ‹–æ…¢æ„å»º
try:
    fitz.TOOLS.mupdf_display_warnings(False)
    fitz.TOOLS.mupdf_display_errors(False)
except Exception:
    pass

# è·³è¿‡æ˜æ˜¾çš„éè®ºæ–‡é™„ä»¶ï¼ˆè¡¥å……ææ–™/å®¡ç¨¿æ–‡ä»¶/ç¿»è¯‘ä»¶ï¼‰
SKIP_FILENAME_PATTERNS = [
    r'(?i)\b(supplement|supplementary|supporting information|transparent peer review|peer review file|supplementary data|supplementary figs?)\b',
    r'(?i)\bsupplemental material\b',
    r'ä¸­æ–‡ç¿»è¯‘',
    r'å…¨æ–‡ä¸­æ–‡ç¿»è¯‘',
    r'ä¸­è¯‘å…¨æ–‡',
    r'ä¸­æ–‡å…¨è¯‘',
    r'è¡¥å……ææ–™',
    r'å›¾åƒå›¾é¢˜ä¸è¡¨é¢˜ä¸­æ–‡ç¿»è¯‘',
]


def should_skip_pdf(filename):
    name = filename.strip()
    for pat in SKIP_FILENAME_PATTERNS:
        if re.search(pat, name):
            return True
    return False


def _is_control_garbled(s):
    return bool(re.search(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', s or ""))


def is_title_low_quality(title):
    """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä½è´¨é‡ï¼ˆä¹±ç /æ¨¡æ¿è¯/å­¦ä½è®ºæ–‡æ³›ç§°ï¼‰"""
    t = (title or "").strip().lower()
    if not t:
        return True
    if _is_control_garbled(t) or 'ï¿½' in t:
        return True
    generic = [
        "article", "research article", "research papers", "original research article",
        "ç¡•å£«å­¦ä½è®ºæ–‡", "åšå£«å­¦ä½è®ºæ–‡", "å­¦ä½è®ºæ–‡"
    ]
    if t in generic:
        return True
    if any(k in t for k in ["copyright", "unauthenticated", "downloaded"]):
        return True
    return False

# ============= è·¯å¾„é…ç½®ï¼ˆä» config.py è¯»å–ï¼Œä¸å­˜åœ¨æ—¶ç”¨é»˜è®¤å€¼ï¼‰=============
try:
    from config import PDF_DIR as BASE_DIR, ZOTERO_DIR, INDEX_PATH as OUTPUT_JSON
    ZOTERO_STORAGE = Path(ZOTERO_DIR).expanduser() / "storage" if ZOTERO_DIR and str(ZOTERO_DIR) else Path("")
    ZOTERO_DB = Path(ZOTERO_DIR).expanduser() / "zotero.sqlite" if ZOTERO_DIR and str(ZOTERO_DIR) else Path("")
    BASE_DIR = Path(BASE_DIR).expanduser()
except ImportError:
    # æœªé…ç½®æ—¶çš„é»˜è®¤å€¼ï¼ˆè¯·ä¿®æ”¹ config.pyï¼‰
    BASE_DIR = Path("~/è®ºæ–‡").expanduser()
    ZOTERO_STORAGE = Path("~/Zotero/storage").expanduser()
    ZOTERO_DB = Path("~/Zotero/zotero.sqlite").expanduser()
    OUTPUT_JSON = Path(__file__).parent / "paper_index.json"
OUTPUT_MD = OUTPUT_JSON.parent / "paper_index_readable.md"

# ============= jieba é¢†åŸŸè¯å…¸ =============
DOMAIN_WORDS = [
    "ç‰©å€™", "ç‰©å€™æœŸ", "ç‰©å€™å˜åŒ–", "è¿”é’æœŸ", "æ¯é»„æœŸ", "ç”Ÿé•¿å­£",
    "è’¸æ•£", "è’¸æ•£å‘", "è’¸è…¾", "æ½œåœ¨è’¸æ•£", "å®é™…è’¸æ•£",
    "å¾„æµ", "äº§æµ", "åŸºæµ", "æ¯æ°´", "æ´ªæ°´", "æ´ªå³°",
    "å¾„æµé‡", "å¾„æµæ·±", "å¾„æµç³»æ•°", "å¤©ç„¶å¾„æµ",
    "æ¤è¢«è¦†ç›–", "æ¤è¢«æ¢å¤", "æ¤è¢«æŒ‡æ•°", "æ¤è¢«åŠ¨æ€",
    "é¥æ„Ÿåæ¼”", "é¥æ„Ÿç›‘æµ‹", "é¥æ„Ÿæ•°æ®",
    "æ°´æºæ¶µå…»", "æ°´æºæ¶µå…»é‡", "æ—å† æˆªç•™",
    "ç”Ÿæ€æµé‡", "ç”Ÿæ€éœ€æ°´", "ç”Ÿæ€åŸºæµ", "ç¯å¢ƒæµé‡",
    "é»„åœŸé«˜åŸ", "è¥¿è¾½æ²³", "æµ·æ²³æµåŸŸ", "é»„æ²³æµåŸŸ", "é•¿æ±ŸæµåŸŸ",
    "æ°”å€™å˜åŒ–", "æ°”å€™å˜æš–", "å…¨çƒå˜æš–", "æç«¯æ°”å€™",
    "é€€è€•è¿˜æ—", "é€€è€•è¿˜è‰", "æ°´åœŸä¿æŒ",
    "æ·¤åœ°å", "é±¼é³å‘", "æ°´å¹³é˜¶", "æ¢¯ç”°",
    "ç¢³å¾ªç¯", "ç¢³æ±‡", "ç¢³å‚¨é‡", "å‡€åˆçº§ç”Ÿäº§åŠ›",
    "å½’å› åˆ†æ", "å¼¹æ€§ç³»æ•°", "æ•æ„Ÿæ€§åˆ†æ",
    "ç»“æ„æ–¹ç¨‹", "é€šå¾„åˆ†æ", "å› æœåˆ†æ",
    "éšæœºæ£®æ—", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ",
    "ç”Ÿæ€æ°´æ–‡", "æ°´æ–‡æ¨¡å‹", "æ°´æ–‡æ•ˆåº”",
    "å¹²æ—±æŒ‡æ•°", "å¹²æ—±äº‹ä»¶", "å¹²æ—±èƒè¿«",
    "æ—¶ç©ºå˜åŒ–", "æ—¶ç©ºæ ¼å±€", "æ—¶ç©ºåˆ†å¸ƒ",
    "æ°´æ–‡æ°”å€™", "æ°´çƒ­è€¦åˆ", "æ°´é‡å¹³è¡¡",
]
for w in DOMAIN_WORDS:
    jieba.add_word(w)

# é¢„åŠ è½½jiebaè¯å…¸ï¼ˆé¿å…é¦–æ¬¡è°ƒç”¨å»¶è¿Ÿï¼‰
jieba.initialize()

# ============= åœç”¨è¯ =============
STOPWORDS_EN = {
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
    'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
    'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
    'would', 'could', 'should', 'may', 'might', 'shall', 'can', 'it',
    'its', 'this', 'that', 'these', 'those', 'we', 'our', 'they', 'their',
    'he', 'she', 'his', 'her', 'not', 'no', 'than', 'more', 'also',
    'between', 'during', 'under', 'over', 'about', 'into', 'through',
    'using', 'used', 'based', 'both', 'each', 'such', 'which', 'where',
    'when', 'how', 'what', 'who', 'whom', 'there', 'here', 'all', 'any',
    'some', 'most', 'very', 'well', 'while', 'however', 'although',
    'because', 'since', 'after', 'before', 'then', 'only', 'just',
    'other', 'new', 'first', 'two', 'three', 'one', 'many', 'much',
    'high', 'low', 'large', 'small', 'long', 'short', 'different',
    'same', 'main', 'major', 'important', 'significant', 'total', 'per',
    'respectively', 'including', 'particularly', 'especially', 'overall',
    'among', 'within', 'without', 'above', 'below', 'across', 'along',
    'further', 'still', 'even', 'thus', 'therefore', 'hence', 'moreover',
    'results', 'study', 'studies', 'research', 'paper', 'analysis',
    'method', 'methods', 'data', 'found', 'showed', 'show', 'shows',
    'indicate', 'indicates', 'indicated', 'suggest', 'suggests',
    'observed', 'compared', 'effect', 'effects', 'impact', 'impacts',
    'increase', 'increased', 'decrease', 'decreased', 'change', 'changes',
    'changed', 'significantly', 'higher', 'lower',
    'university', 'institute', 'department', 'college', 'school',
    'laboratory', 'center', 'journal', 'proceedings', 'press',
    'beijing', 'china', 'usa', 'doi', 'http', 'https', 'www',
    'fig', 'figure', 'table', 'section', 'abstract', 'keywords',
    'acknowledgement', 'acknowledgements', 'references', 'appendix',
}
STOPWORDS_ZH = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'å¯¹', 'åŠ', 'ç­‰', 'ä¸º', 'ä¸­',
    'ä¸Š', 'ä¸‹', 'æœ‰', 'æ— ', 'ä¸', 'ä¹Ÿ', 'åˆ', 'è¢«', 'æˆ–', 'å°†', 'æŠŠ',
    'ä»', 'åˆ°', 'ä»¥', 'ç”¨', 'å¯', 'èƒ½', 'ä¼š', 'è¦', 'å°±', 'éƒ½', 'è€Œ',
    'ä½†', 'è¿™', 'é‚£', 'å…¶', 'ä¹‹', 'æ‰€', 'è€…', 'æ­¤', 'ä¸ª', 'å·²', 'ç”±',
    'äº', 'åˆ™', 'å¹¶', 'ä¸”', 'å¦‚', 'è¿›è¡Œ', 'é€šè¿‡', 'åˆ©ç”¨', 'é‡‡ç”¨', 'åˆ†æ',
    'ç ”ç©¶', 'ç»“æœ', 'è¡¨æ˜', 'æ˜¾ç¤º', 'æå‡º', 'æé«˜', 'åŸºäº', 'æ–¹æ³•',
    'å½±å“', 'å˜åŒ–', 'æ¡ä»¶', 'ä¸åŒ', 'æƒ…å†µ', 'å…³ç³»', 'ä½œç”¨', 'å…·æœ‰',
    'ç›¸å…³', 'è¾ƒå¤§', 'è¾ƒå°', 'æ˜æ˜¾', 'ä¸»è¦', 'ä¸€å®š', 'åŒæ—¶', 'ä»¥åŠ',
    'å¤§å­¦', 'å­¦é™¢', 'å­¦æŠ¥', 'æ•™æˆ', 'åšå£«', 'ç¡•å£«', 'å¯¼å¸ˆ', 'ä½œè€…',
    'åŒ—äº¬', 'ä¸Šæµ·', 'å—äº¬', 'ä¸­å›½', 'å·¥ç¨‹', 'å­¦ä½', 'è®ºæ–‡', 'ä¸“ä¸š',
    'ç§‘å­¦', 'ç§‘å­¦é™¢', 'ç ”ç©¶æ‰€', 'ç ”ç©¶é™¢', 'å®éªŒå®¤', 'ä¸­å¿ƒ',
    'ä¸­æ–‡', 'è‹±æ–‡', 'ç¿»è¯‘', 'å…¨æ–‡', 'æ‘˜è¦', 'å…³é”®', 'å‚è€ƒ', 'æ–‡çŒ®',
}

# OCR æ”¯æŒï¼ˆå¯é€‰ï¼‰
try:
    import pytesseract
    from PIL import Image
    import io
    HAS_OCR = True
except ImportError:
    HAS_OCR = False


# ============= v3: jiebaåˆ†è¯ =============
def tokenize(text):
    """åˆ†è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰- v3ä½¿ç”¨jiebaåˆ‡è¯ï¼Œæ›¿ä»£n-gram"""
    tokens = set()

    # è‹±æ–‡å•è¯
    en_words = re.findall(r'[a-zA-Z][a-zA-Z0-9_-]{1,}', text)
    tokens.update(w.lower() for w in en_words if len(w) >= 2)

    # ä¸­æ–‡ï¼šjiebaæœç´¢æ¨¡å¼åˆ†è¯ï¼ˆå…¼é¡¾é•¿çŸ­è¯ï¼‰
    zh_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
    if zh_text:
        words = jieba.cut_for_search(zh_text)
        tokens.update(w for w in words if len(w) >= 2 and w not in STOPWORDS_ZH)

    return sorted(tokens)


# ============= v3: Zoteroå…ƒæ•°æ® =============
def load_zotero_metadata(db_path):
    """ä»Zoteroæ•°æ®åº“è¯»å–æ¯ç¯‡PDFçš„å…ƒæ•°æ®
    è¿”å› {filename: {title, authors, collections, tags, year}}
    """
    if not db_path.exists():
        print(f"  Zoteroæ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        return {}

    try:
        conn = sqlite3.connect(f'file:{db_path}?mode=ro', uri=True)
    except Exception:
        # å¦‚æœåªè¯»æ¨¡å¼å¤±è´¥ï¼Œå¤åˆ¶åˆ°ä¸´æ—¶æ–‡ä»¶
        import shutil, tempfile
        tmp = Path(tempfile.mktemp(suffix='.sqlite'))
        shutil.copy2(db_path, tmp)
        conn = sqlite3.connect(str(tmp))

    cur = conn.cursor()
    metadata = {}

    # è·å–æ‰€æœ‰PDFé™„ä»¶åŠå…¶çˆ¶æ¡ç›®ID
    cur.execute('''
        SELECT ia.path, ia.parentItemID
        FROM itemAttachments ia
        WHERE ia.contentType = 'application/pdf' AND ia.path IS NOT NULL
    ''')
    pdf_items = cur.fetchall()

    # è·å–å­—æ®µIDæ˜ å°„
    cur.execute("SELECT fieldID, fieldName FROM fields")
    field_map = {name: fid for fid, name in cur.fetchall()}
    title_fid = field_map.get('title')
    date_fid = field_map.get('date')

    for path, parent_id in pdf_items:
        if not path or not path.startswith('storage:'):
            continue
        filename = path[len('storage:'):]
        if not filename.endswith('.pdf'):
            continue

        meta = {'title': '', 'authors': [], 'collections': [], 'tags': [], 'year': ''}

        if parent_id:
            # æ ‡é¢˜
            if title_fid:
                cur.execute('''
                    SELECT idv.value FROM itemData id
                    JOIN itemDataValues idv ON id.valueID = idv.valueID
                    WHERE id.itemID = ? AND id.fieldID = ?
                ''', (parent_id, title_fid))
                row = cur.fetchone()
                if row:
                    meta['title'] = row[0]

            # å¹´ä»½
            if date_fid:
                cur.execute('''
                    SELECT idv.value FROM itemData id
                    JOIN itemDataValues idv ON id.valueID = idv.valueID
                    WHERE id.itemID = ? AND id.fieldID = ?
                ''', (parent_id, date_fid))
                row = cur.fetchone()
                if row:
                    m = re.search(r'(19|20)\d{2}', str(row[0]))
                    if m:
                        meta['year'] = m.group(0)

            # ä½œè€…
            cur.execute('''
                SELECT c.firstName, c.lastName FROM itemCreators ic
                JOIN creators c ON ic.creatorID = c.creatorID
                WHERE ic.itemID = ? ORDER BY ic.orderIndex
            ''', (parent_id,))
            meta['authors'] = [f"{r[1]} {r[0]}".strip() for r in cur.fetchall()]

            # åˆ†ç±»ï¼ˆcollectionsï¼‰
            cur.execute('''
                SELECT c.collectionName FROM collectionItems ci
                JOIN collections c ON ci.collectionID = c.collectionID
                WHERE ci.itemID = ?
            ''', (parent_id,))
            meta['collections'] = [r[0] for r in cur.fetchall()]

            # æ ‡ç­¾
            cur.execute('''
                SELECT t.name FROM itemTags it
                JOIN tags t ON it.tagID = t.tagID
                WHERE it.itemID = ?
            ''', (parent_id,))
            meta['tags'] = [r[0] for r in cur.fetchall()]

        metadata[filename] = meta

    conn.close()
    return metadata


# ============= PDFæå–å‡½æ•°ï¼ˆä¿ç•™v2å…¨éƒ¨é€»è¾‘ï¼‰ =============

def auto_generate_keywords(abstract, first_pages_text, lang):
    """ä»æ‘˜è¦å’Œé¦–é¡µæ–‡æœ¬è‡ªåŠ¨ç”Ÿæˆå…³é”®è¯"""
    clean_abstract = re.sub(r'^\[å…œåº•æå–\]\s*', '', abstract or "")
    text = clean_abstract + " " + (first_pages_text or "")[:1000]
    if len(text.strip()) < 50:
        return ""

    if lang == "zh":
        segments = re.findall(r'[\u4e00-\u9fff]{2,6}', text)
        freq = Counter(segments)
        keywords = []
        seen = set()
        for word, cnt in freq.most_common(50):
            if word in STOPWORDS_ZH or len(word) < 2:
                continue
            if any(word in kw for kw in seen if len(kw) > len(word)):
                continue
            seen.add(word)
            keywords.append(word)
            if len(keywords) >= 8:
                break
        return "[è‡ªåŠ¨] " + "; ".join(keywords) if keywords else ""
    else:
        words = re.findall(r'[a-zA-Z][a-zA-Z-]{2,}', text)
        freq = Counter(w.lower() for w in words)
        keywords = []
        for word, cnt in freq.most_common(60):
            if word in STOPWORDS_EN or len(word) < 3 or cnt < 2:
                continue
            keywords.append(word)
            if len(keywords) >= 8:
                break
        bigrams = []
        words_list = [w.lower() for w in words if w.lower() not in STOPWORDS_EN and len(w) > 2]
        for i in range(len(words_list) - 1):
            bg = f"{words_list[i]} {words_list[i+1]}"
            bigrams.append(bg)
        bg_freq = Counter(bigrams)
        top_bigrams = [bg for bg, cnt in bg_freq.most_common(5) if cnt >= 2]
        combined = top_bigrams[:4] + [kw for kw in keywords if not any(kw in bg for bg in top_bigrams)]
        return "[auto] " + "; ".join(combined[:8]) if combined else ""


def detect_language(text):
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total = len(text.strip())
    if total == 0:
        return "unknown"
    return "zh" if chinese_chars / total > 0.15 else "en"


def is_thesis(text, page_count):
    if page_count < 30:
        return False
    thesis_markers = [
        'å­¦ä½è®ºæ–‡', 'ç¡•å£«', 'åšå£«', 'å­¦ä½ç±»åˆ«', 'æŒ‡å¯¼æ•™å¸ˆ', 'å¯¼å¸ˆ',
        'è®ºæ–‡ç­”è¾©', 'å­¦ç§‘ä¸“ä¸š', 'ç ”ç©¶æ–¹å‘', 'è®ºæ–‡æäº¤',
        'Thesis', 'Dissertation', 'degree', 'supervisor',
        'Submitted to', 'Fulfillment', 'Requirements for',
    ]
    text_lower = text.lower()
    matches = sum(1 for m in thesis_markers if m.lower() in text_lower)
    return matches >= 2


def extract_abstract_zh(text):
    patterns = [
        r'æ‘˜\s*è¦[ï¼š:\s]*(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—|Key\s*words|Keywords)',
        r'ä¸­æ–‡æ‘˜è¦[ï¼š:\s\d]*(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—|Key\s*words|Keywords|è‹±æ–‡æ‘˜è¦|Abstract|ABSTRACT)',
        r'å†…å®¹æ‘˜è¦[ï¼š:\s]*(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—)',
        r'å†…å®¹æè¦[ï¼š:\s]*(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—)',
        r'æ‘˜è¦[ï¼ˆ(]\s*Abstract\s*[)ï¼‰][ï¼š:\s]*(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—|Key\s*words)',
        r'æ‘˜\s*è¦[ï¼š:\s]*(.+?)(?:ABSTRACT|Abstract\s)',
        r'æ‘˜\s*è¦\s*[Iâ… ]?\s*\n(.+?)(?:å…³\s*é”®\s*è¯|å…³é”®å­—|Key\s*words)',
        r'æ‘˜\s*è¦[ï¼š:\s]*(.{200,}?)(?:\n\s*(?:ç›®\s*å½•|ç¬¬\s*[ä¸€äºŒä¸‰1-3]\s*ç« |1\s+å¼•è¨€|1\s+ç»ªè®º|å‰\s*è¨€))',
    ]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL | re.IGNORECASE)
        if m:
            abstract = m.group(1).strip()
            abstract = re.sub(r'\s+', ' ', abstract)
            if len(abstract) > 50:
                return abstract[:2000]
    return ""


def extract_abstract_en(text):
    terminators = (
        r'(?:'
        r'K\s*E\s*Y\s*W\s*O\s*R\s*D\s*S'
        r'|Keywords?|Key\s*words|INDEX\s+TERMS|Index\s+terms'
        r'|Introduction|1\s*\.?\s*Introduction|I\.\s+Introduction'
        r'|Background\s*(?:&|and)\s*Summary'
        r'|Results?\s*(?:&|and)\s*Discussion'
        r'|Main\s*\n'
        r'|\n\s*\n\s*(?:\d+\.?\s+\w)'
        r'|Â©|Copyright'
        r')'
    )
    patterns = [
        rf'Abstract[.:\sâ€”â€“-]*(.+?)(?:{terminators})',
        rf'A\s*B\s*S\s*T\s*R\s*A\s*C\s*T[:\s]*(.+?)(?:{terminators})',
        rf'SUMMARY[:\s]*(.+?)(?:{terminators})',
        rf'(?:Highlights?.+?)Abstract[.:\s]*(.+?)(?:{terminators})',
        r'Abstract[.:\sâ€”â€“-]*(.{200,2000?}?)(?:\n\s*\n)',
    ]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL | re.IGNORECASE)
        if m:
            abstract = m.group(1).strip()
            abstract = re.sub(r'\s+', ' ', abstract)
            if len(abstract) > 50:
                return abstract[:2000]
    return ""


def extract_fallback_abstract(text, lang):
    skip_markers = [
        'http', 'doi:', 'issn', 'Â©', 'copyright', 'received',
        'æ”¶ç¨¿æ—¥æœŸ', 'åŸºé‡‘é¡¹ç›®', 'ä½œè€…ç®€ä»‹', 'ç›®å½•', 'å‚è€ƒæ–‡çŒ®',
        'å‘æ˜ä¸“åˆ©', 'æƒåˆ©è¦æ±‚', 'cite this', 'accepted',
        'published online', 'supplementary', 'supplement of',
    ]

    def is_content_paragraph(clean, lang, strict_skip=True):
        if len(clean) < 150:
            return False
        check_text = clean.lower() if strict_skip else clean[:100].lower()
        if any(m in check_text for m in skip_markers):
            return False
        if lang == "zh":
            return len(re.findall(r'[\u4e00-\u9fff]', clean)) > 50
        else:
            words = clean.split()
            return len(words) > 40 and any(c == '.' for c in clean)

    lines = text.split('\n')
    affiliation_prefix_re = re.compile(r'^[a-z]{1,2}[A-Z]')
    non_content_re = re.compile(
        r'(?:University|Institute|Department|College|Academy|Laboratory)'
        r'|(?:Edited by|Contributed by|Communicated by|Significance)'
        r'|(?:@|\.edu|\.ac\.|\.org)'
        r'|(?:\d{5})'
        r'|(?:Received |Accepted |Available online|Published online)',
        re.IGNORECASE
    )
    found_start = None
    for i, line in enumerate(lines[:50]):
        stripped = line.strip()
        if len(stripped) < 40:
            continue
        if non_content_re.search(stripped) or affiliation_prefix_re.match(stripped):
            continue
        if re.match(r'^[A-Z][a-z]', stripped) and '.' not in stripped[:5]:
            found_start = i
            break
    if found_start is not None:
        candidate = []
        for j in range(found_start, min(found_start + 25, len(lines))):
            l = lines[j].strip()
            if re.match(r'^(?:Introduction|Background\b|Results|Methods|Main$|1\s*[\.\s])', l, re.IGNORECASE):
                break
            if re.match(r'^(?:Background\s*(?:&|and)\s*Summary)', l, re.IGNORECASE):
                break
            if re.match(r'^(?:water cycle|Keywords?|K\s*E\s*Y)', l, re.IGNORECASE) and len(l) < 200:
                break
            candidate.append(l)
        joined = re.sub(r'\s+', ' ', ' '.join(candidate)).strip()
        if is_content_paragraph(joined, lang, strict_skip=False):
            return "[å…œåº•æå–] " + joined[:2000]

    paragraphs = re.split(r'\n\s*\n', text)
    for para in paragraphs:
        clean = re.sub(r'\s+', ' ', para).strip()
        if is_content_paragraph(clean, lang):
            return "[å…œåº•æå–] " + clean[:2000]

    current_block = []
    for line in lines:
        stripped = line.strip()
        if not stripped:
            if current_block:
                joined = re.sub(r'\s+', ' ', ' '.join(current_block)).strip()
                if is_content_paragraph(joined, lang):
                    return "[å…œåº•æå–] " + joined[:2000]
                current_block = []
        else:
            current_block.append(stripped)
    if current_block:
        joined = re.sub(r'\s+', ' ', ' '.join(current_block)).strip()
        if is_content_paragraph(joined, lang):
            return "[å…œåº•æå–] " + joined[:2000]

    full_text = re.sub(r'\s+', ' ', text).strip()
    sentences = re.split(r'(?<=[.ã€‚])\s+', full_text)
    consecutive = []
    for sent in sentences:
        sent = sent.strip()
        if len(sent) < 30:
            if consecutive:
                break
            continue
        if any(m in sent.lower() for m in skip_markers):
            if consecutive:
                break
            continue
        if re.match(r'^[a-z](?:Department|University|Institute|School)', sent):
            continue
        consecutive.append(sent)
        if len(consecutive) >= 3:
            joined = '. '.join(consecutive)
            if len(joined) > 200:
                return "[å…œåº•æå–] " + joined[:2000]
    return ""


def extract_keywords_zh(text):
    patterns = [
        r'å…³\s*é”®\s*è¯[ï¼š:\s]*(.+?)(?:\n|Abstract|ABSTRACT|ä¸­å›¾åˆ†ç±»å·|åˆ†ç±»å·|$)',
    ]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL | re.IGNORECASE)
        if m:
            kw = m.group(1).strip()
            kw = re.sub(r'\s+', ' ', kw)
            return kw[:500]
    return ""


def extract_keywords_en(text):
    patterns = [
        r'Key\s*words?[ï¼š:\s]*(.+?)(?:\n\n|\n\s*\d|Introduction|Copyright|Â©|$)',
        r'KEY\s*WORDS?[ï¼š:\s]*(.+?)(?:\n\n|\n\s*\d|Introduction|$)',
        r'K\s*E\s*Y\s*W\s*O\s*R\s*D\s*S\s*\n(.+?)(?:\n\s*\n|\n\s*\d|\n\s*(?:INTRODUCTION|Introduction))',
        r'Index\s+terms?[ï¼š:\s]*(.+?)(?:\n\n|\n\s*\d|Introduction|$)',
    ]
    for pat in patterns:
        m = re.search(pat, text, re.DOTALL | re.IGNORECASE)
        if m:
            kw = m.group(1).strip()
            kw = re.sub(r'\s+', ' ', kw)
            if len(kw) > 3:
                return kw[:500]
    return ""


def extract_keywords(text, lang):
    kw_zh = extract_keywords_zh(text)
    kw_en = extract_keywords_en(text)
    if kw_zh and kw_en:
        return f"{kw_zh} | {kw_en}"
    return kw_zh or kw_en


def extract_year(filename, text):
    m = re.search(r'(19|20)\d{2}', filename)
    if m:
        return m.group(0)
    years = re.findall(r'(20[0-2]\d|19\d{2})', text[:3000])
    if years:
        common = Counter(years).most_common(3)
        for y, _ in common:
            if 1990 <= int(y) <= 2026:
                return y
    return ""


def extract_title_from_text(text, lang):
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    if not lines:
        return ""
    skip_words = [
        'doi:', 'http', 'journal', 'volume', 'issn', 'æ”¶ç¨¿æ—¥æœŸ',
        'åŸºé‡‘é¡¹ç›®', 'ä½œè€…ç®€ä»‹', 'åˆ†ç±»å·', 'å¯†çº§', 'ç¼–å·',
        'scientific data', 'scientific reports', 'www.nature.com',
    ]
    for line in lines[:8]:
        if any(skip in line.lower() for skip in skip_words):
            continue
        if len(line) > 5 and len(line) < 200:
            return line[:200]
    return ""


def ocr_scanned_pdf(doc, max_pages=3):
    if not HAS_OCR:
        return ""
    text_parts = []
    for i in range(min(max_pages, len(doc))):
        page = doc[i]
        mat = fitz.Matrix(2, 2)
        pix = page.get_pixmap(matrix=mat)
        img = Image.open(io.BytesIO(pix.tobytes("png")))
        try:
            langs = 'chi_sim+eng' if os.path.exists('/usr/share/tesseract-ocr/5/tessdata/chi_sim.traineddata') else 'eng'
            page_text = pytesseract.image_to_string(img, lang=langs)
            text_parts.append(page_text)
        except Exception:
            pass
    return "\n".join(text_parts)


# ============= æ ¸å¿ƒå¤„ç†å‡½æ•° =============

def process_pdf(pdf_path, source="local", zotero_meta=None):
    """å¤„ç†å•ä¸ªPDFï¼Œæå–å…ƒæ•°æ®
    source: "local" æˆ– "zotero"
    zotero_meta: Zoteroå…ƒæ•°æ®å­—å…¸ï¼ˆä»…zoteroæ¥æºï¼‰
    """
    # ç¡®å®šfolderå­—æ®µ
    if source == "local":
        try:
            folder = str(pdf_path.parent.relative_to(BASE_DIR))
        except ValueError:
            folder = pdf_path.parent.name
    elif source == "zotero" and zotero_meta:
        if zotero_meta.get("collections"):
            folder = " | ".join(zotero_meta["collections"])
        else:
            folder = "Zotero/æœªåˆ†ç±»"
    else:
        folder = "unknown"

    result = {
        "path": str(pdf_path),
        "folder": folder,
        "filename": pdf_path.name,
        "source": source,
        "language": "unknown",
        "title_extracted": "",
        "abstract": "",
        "keywords": "",
        "year": "",
        "first_pages_text": "",
        "text_length": 0,
        "page_count": 0,
        "is_scannable": False,
        "is_thesis": False,
        "extraction_method": "standard",
    }

    # v3: å†™å…¥Zoteroå…ƒæ•°æ®
    if zotero_meta:
        result["zotero_title"] = zotero_meta.get("title", "")
        result["zotero_authors"] = zotero_meta.get("authors", [])
        result["zotero_collections"] = zotero_meta.get("collections", [])
        result["zotero_tags"] = zotero_meta.get("tags", [])
        # Zoteroå¹´ä»½ä¼˜å…ˆ
        if zotero_meta.get("year"):
            result["year"] = zotero_meta["year"]

    try:
        doc = fitz.open(str(pdf_path))
        result["page_count"] = len(doc)

        text_3p = ""
        for i in range(min(3, len(doc))):
            text_3p += doc[i].get_text() + "\n"

        if len(text_3p.strip()) < 100:
            result["is_scannable"] = True
            ocr_text = ocr_scanned_pdf(doc, max_pages=3)
            if len(ocr_text.strip()) > 100:
                text_3p = ocr_text
                result["is_scannable"] = False
                result["extraction_method"] = "ocr"
            else:
                doc.close()
                return result

        thesis = is_thesis(text_3p, result["page_count"])
        result["is_thesis"] = thesis

        if thesis:
            text = ""
            for i in range(min(10, len(doc))):
                text += doc[i].get_text() + "\n"
            result["extraction_method"] = "extended"
        else:
            text = text_3p

        result["text_length"] = len(text.strip())

        lang = detect_language(text)
        result["language"] = lang

        if lang == "zh":
            abstract = extract_abstract_zh(text)
        else:
            abstract = extract_abstract_en(text)

        if not abstract:
            if lang == "zh":
                abstract = extract_abstract_en(text)
            else:
                abstract = extract_abstract_zh(text)

        if not abstract:
            abstract = extract_fallback_abstract(text, lang)
            if abstract:
                result["extraction_method"] = "fallback"

        result["abstract"] = abstract
        result["keywords"] = extract_keywords(text, lang)

        if not result["year"]:
            result["year"] = extract_year(pdf_path.name, text)

        result["title_extracted"] = extract_title_from_text(text, lang)

        clean_text = re.sub(r'\s+', ' ', text)
        max_text = 6000 if thesis else 3000
        result["first_pages_text"] = clean_text[:max_text]

        doc.close()

        # è‹¥æå–æ ‡é¢˜è´¨é‡è¾ƒå·®ï¼Œä¼˜å…ˆä½¿ç”¨Zoteroæ ‡é¢˜è¦†ç›–
        if result.get("zotero_title") and is_title_low_quality(result.get("title_extracted", "")):
            result["title_extracted"] = result["zotero_title"][:200]

        # æ‘˜è¦å…œåº•è¡¥å…¨ï¼šä¼˜å…ˆé¦–æ®µæ­£æ–‡ï¼Œå…¶æ¬¡Zoteroæ ‡é¢˜/æ–‡ä»¶å
        if not result["abstract"]:
            if result.get("first_pages_text"):
                snippet = result["first_pages_text"][:600].strip()
                if snippet:
                    result["abstract"] = f"[å…œåº•è¡¥å…¨] {snippet}"
            if not result["abstract"] and result.get("zotero_title"):
                result["abstract"] = f"[å…œåº•è¡¥å…¨] {result['zotero_title']}"
            if not result["abstract"]:
                result["abstract"] = f"[å…œåº•è¡¥å…¨] {result['filename']}"
            if result["abstract"] and result.get("extraction_method") == "standard":
                result["extraction_method"] = "fallback"

        # å…³é”®è¯å…œåº•è¡¥å…¨
        if not result["keywords"]:
            result["keywords"] = auto_generate_keywords(
                result.get("abstract", ""), result.get("first_pages_text", ""), lang
            )
        if not result["keywords"] and result.get("zotero_title"):
            toks = tokenize(result["zotero_title"])[:8]
            if toks:
                result["keywords"] = "[è‡ªåŠ¨] " + "; ".join(toks)
        if not result["keywords"]:
            fallback_title = result.get("zotero_title") or result.get("title_extracted") or result.get("filename", "")
            toks = tokenize(fallback_title)[:8]
            if toks:
                result["keywords"] = "[è‡ªåŠ¨] " + "; ".join(toks)
        if not result["keywords"]:
            fallback_text = f"{result.get('title_extracted', '')} {result.get('filename', '')}"
            zh_chunks = re.findall(r'[\u4e00-\u9fff]{2,8}', fallback_text)
            if zh_chunks:
                result["keywords"] = "[è‡ªåŠ¨] " + "; ".join(list(dict.fromkeys(zh_chunks))[:8])

        # v3: é¢„è®¡ç®—tokensï¼ˆä½¿ç”¨jiebaåˆ†è¯ï¼‰
        result["tokens"] = {}
        for field in ("filename", "keywords", "abstract", "title_extracted", "folder"):
            val = result.get(field, "")
            if val:
                result["tokens"][field] = tokenize(val)
        # v3: é¢å¤–ç´¢å¼•Zoteroå…ƒæ•°æ®
        zotero_text_parts = []
        if result.get("zotero_title"):
            zotero_text_parts.append(result["zotero_title"])
        if result.get("zotero_tags"):
            zotero_text_parts.append(" ".join(result["zotero_tags"]))
        if result.get("zotero_collections"):
            zotero_text_parts.append(" ".join(result["zotero_collections"]))
        if zotero_text_parts:
            result["tokens"]["zotero_meta"] = tokenize(" ".join(zotero_text_parts))

    except Exception as e:
        result["error"] = str(e)

    return result


# ============= ç´¢å¼•æ„å»º =============

def build_index(incremental=False, include_zotero=True):
    """æ„å»ºå®Œæ•´ç´¢å¼•"""
    print(f"=== æ–‡çŒ®ç´¢å¼•æ„å»ºå™¨ v3 ===")

    # 1. æ‰«ææœ¬åœ°ç›®å½•
    print(f"\n[1/4] æ‰«ææœ¬åœ°ç›®å½•: {BASE_DIR}")
    local_all = sorted(BASE_DIR.rglob("*.pdf"))
    local_pdfs = [p for p in local_all if not should_skip_pdf(p.name)]
    local_skipped = len(local_all) - len(local_pdfs)
    print(f"  æœ¬åœ°PDF: {len(local_pdfs)}ç¯‡" + (f" (è·³è¿‡é™„ä»¶{local_skipped}ç¯‡)" if local_skipped else ""))

    # 2. æ‰«æZoteroç›®å½•
    zotero_pdfs = []
    zotero_meta_map = {}
    if include_zotero and ZOTERO_STORAGE.exists():
        print(f"\n[2/4] æ‰«æZoteroç›®å½•: {ZOTERO_STORAGE}")
        zotero_all = sorted(ZOTERO_STORAGE.rglob("*.pdf"))
        zotero_pdfs = [p for p in zotero_all if not should_skip_pdf(p.name)]
        zotero_skipped = len(zotero_all) - len(zotero_pdfs)
        print(f"  Zotero PDF: {len(zotero_pdfs)}ç¯‡" + (f" (è·³è¿‡é™„ä»¶{zotero_skipped}ç¯‡)" if zotero_skipped else ""))

        # è¯»å–Zoteroå…ƒæ•°æ®
        print(f"  è¯»å–Zoteroæ•°æ®åº“å…ƒæ•°æ®...")
        zotero_meta_map = load_zotero_metadata(ZOTERO_DB)
        print(f"  è·å–åˆ° {len(zotero_meta_map)} ç¯‡å…ƒæ•°æ®")
    else:
        print(f"\n[2/4] è·³è¿‡Zoteroæ‰«æ")

    # 3. å»é‡ï¼šæœ¬åœ°ä¼˜å…ˆ
    local_filenames = {p.name for p in local_pdfs}
    zotero_unique = [p for p in zotero_pdfs if p.name not in local_filenames]
    duplicates = len(zotero_pdfs) - len(zotero_unique)
    all_pdfs_info = [(p, "local", None) for p in local_pdfs]
    for p in zotero_unique:
        meta = zotero_meta_map.get(p.name, None)
        all_pdfs_info.append((p, "zotero", meta))

    # åŒæ—¶ä¸ºæœ¬åœ°PDFè¡¥å……Zoteroå…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    for i, (p, source, meta) in enumerate(all_pdfs_info):
        if source == "local" and p.name in zotero_meta_map:
            all_pdfs_info[i] = (p, "local", zotero_meta_map[p.name])

    total = len(all_pdfs_info)
    print(f"\n[3/4] å»é‡åæ€»è®¡: {total}ç¯‡ (æœ¬åœ°{len(local_pdfs)} + Zoteroç‹¬æœ‰{len(zotero_unique)}, é‡å¤è·³è¿‡{duplicates})")

    # å¢é‡æ¨¡å¼å¤„ç†
    existing_papers = []
    existing_filenames = set()
    if incremental and OUTPUT_JSON.exists():
        with open(OUTPUT_JSON, "r", encoding="utf-8") as f:
            old_index = json.load(f)
            existing_papers = old_index.get("papers", [])
            existing_filenames = {p["filename"] for p in existing_papers}

        # ç§»é™¤ä¸å†å­˜åœ¨çš„æ–‡ä»¶
        current_filenames = {info[0].name for info in all_pdfs_info}
        removed = existing_filenames - current_filenames
        if removed:
            existing_papers = [p for p in existing_papers if p["filename"] not in removed]
            print(f"  ç§»é™¤ {len(removed)} ä¸ªå·²åˆ é™¤æ–‡ä»¶çš„ç´¢å¼•")

        new_pdfs_info = [info for info in all_pdfs_info if info[0].name not in existing_filenames]
        print(f"  å¢é‡æ¨¡å¼: è·³è¿‡ {total - len(new_pdfs_info)} ä¸ªå·²ç´¢å¼•, å¤„ç† {len(new_pdfs_info)} ä¸ªæ–°æ–‡ä»¶")
        all_pdfs_info = new_pdfs_info
    else:
        existing_papers = []

    # 4. å¤„ç†æ‰€æœ‰PDF
    print(f"\n[4/4] å¤„ç†PDF...")
    papers = list(existing_papers)
    errors = []
    scanned = []
    start_time = time.time()

    for i, (pdf_path, source, meta) in enumerate(all_pdfs_info):
        if (i + 1) % 50 == 0 or (i + 1) == len(all_pdfs_info):
            elapsed = time.time() - start_time
            speed = (i + 1) / elapsed if elapsed > 0 else 0
            eta = (len(all_pdfs_info) - i - 1) / speed if speed > 0 else 0
            print(f"  è¿›åº¦: {i+1}/{len(all_pdfs_info)} ({elapsed:.0f}s, {speed:.1f}ç¯‡/s, ETA {eta:.0f}s)")

        result = process_pdf(pdf_path, source=source, zotero_meta=meta)
        papers.append(result)

        if result.get("error"):
            errors.append(result["filename"])
        if result.get("is_scannable"):
            scanned.append(result["filename"])

    elapsed = time.time() - start_time

    # æœ€ç»ˆå»é‡ï¼ˆæŒ‰æ–‡ä»¶åï¼Œä¿ç•™ç¬¬ä¸€ä¸ª=æœ¬åœ°ä¼˜å…ˆï¼‰
    # v3.1: æ¨¡ç³Šå»é‡ - å»æ‰"è®ºæ–‡53-"ã€"39."ç­‰ç¼–å·å‰ç¼€åæ¯”è¾ƒ
    def normalize_filename(name):
        """æ ‡å‡†åŒ–æ–‡ä»¶åç”¨äºå»é‡æ¯”è¾ƒ"""
        # å»æ‰ 'è®ºæ–‡53-' '39.' '11.' ç­‰å‰ç¼€ç¼–å·
        name = re.sub(r'^(?:è®ºæ–‡)?\d+[\.\-\s]+', '', name)
        # å»æ‰å¤šä½™ç©ºæ ¼
        name = name.replace(' ', '')
        return name

    seen_filenames = set()     # åŸå§‹æ–‡ä»¶åå»é‡
    seen_normalized = set()    # æ ‡å‡†åŒ–æ–‡ä»¶åå»é‡
    deduped = []
    fuzzy_dupes = 0
    for p in papers:
        if p["filename"] in seen_filenames:
            continue
        norm = normalize_filename(p["filename"])
        if norm in seen_normalized:
            fuzzy_dupes += 1
            continue
        seen_filenames.add(p["filename"])
        seen_normalized.add(norm)
        deduped.append(p)
    papers = deduped
    if fuzzy_dupes:
        print(f"  æ¨¡ç³Šå»é‡: ç§»é™¤ {fuzzy_dupes} ç¯‡ç¼–å·å‰ç¼€é‡å¤è®ºæ–‡")

    # ç»Ÿè®¡
    stats = {
        "total_papers": len(papers),
        "local_papers": sum(1 for p in papers if p.get("source") == "local"),
        "zotero_papers": sum(1 for p in papers if p.get("source") == "zotero"),
        "with_abstract": sum(1 for p in papers if p["abstract"]),
        "with_keywords": sum(1 for p in papers if p["keywords"]),
        "chinese_papers": sum(1 for p in papers if p["language"] == "zh"),
        "english_papers": sum(1 for p in papers if p["language"] == "en"),
        "thesis_papers": sum(1 for p in papers if p.get("is_thesis")),
        "scanned_papers": sum(1 for p in papers if p.get("is_scannable")),
        "error_papers": sum(1 for p in papers if p.get("error")),
        "with_zotero_meta": sum(1 for p in papers if p.get("zotero_title")),
        "by_method": {
            "standard": sum(1 for p in papers if p.get("extraction_method") == "standard"),
            "extended": sum(1 for p in papers if p.get("extraction_method") == "extended"),
            "fallback": sum(1 for p in papers if p.get("extraction_method") == "fallback"),
            "ocr": sum(1 for p in papers if p.get("extraction_method") == "ocr"),
        },
        "abstract_with_fallback": sum(1 for p in papers if p["abstract"] and "[å…œåº•æå–]" in p["abstract"]),
        "keywords_auto": sum(1 for p in papers if p["keywords"] and (p["keywords"].startswith("[è‡ªåŠ¨]") or p["keywords"].startswith("[auto]"))),
        "build_time_seconds": round(elapsed, 1),
        "build_date": time.strftime("%Y-%m-%d %H:%M:%S"),
        "version": "v3",
    }

    # é«˜é¢‘å…³é”®è¯
    all_keywords_raw = []
    for p in papers:
        if p["keywords"]:
            kws = re.split(r'[;ï¼›,ï¼Œ|]+', p["keywords"])
            all_keywords_raw.extend(kw.strip().lower() for kw in kws if kw.strip())
    keyword_freq = Counter(all_keywords_raw).most_common(100)
    stats["top_keywords"] = [{"keyword": kw, "count": cnt} for kw, cnt in keyword_freq]

    index = {
        "stats": stats,
        "papers": papers,
        "scanned_files": [p["filename"] for p in papers if p.get("is_scannable")],
        "error_files": [p["filename"] for p in papers if p.get("error")],
    }

    # ä¿å­˜
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(index, f, ensure_ascii=False, indent=1)

    json_size = os.path.getsize(OUTPUT_JSON)
    print(f"\n=== æ„å»ºå®Œæˆ ===")
    print(f"æ€»è€—æ—¶: {elapsed:.1f}s")
    print(f"ç´¢å¼•æ–‡ä»¶: {OUTPUT_JSON} ({json_size/1024/1024:.1f} MB)")
    print(f"æ€»è®ºæ–‡: {stats['total_papers']} (æœ¬åœ°{stats['local_papers']} + Zotero{stats['zotero_papers']})")
    print(f"æœ‰Zoteroå…ƒæ•°æ®: {stats['with_zotero_meta']}")
    print(f"æœ‰æ‘˜è¦: {stats['with_abstract']} ({stats['with_abstract']*100//max(stats['total_papers'],1)}%)")
    print(f"  å…¶ä¸­å…œåº•æå–: {stats['abstract_with_fallback']}")
    print(f"æœ‰å…³é”®è¯: {stats['with_keywords']} ({stats['with_keywords']*100//max(stats['total_papers'],1)}%)")
    print(f"  å…¶ä¸­è‡ªåŠ¨ç”Ÿæˆ: {stats['keywords_auto']}")
    print(f"ä¸­æ–‡: {stats['chinese_papers']}, è‹±æ–‡: {stats['english_papers']}")
    print(f"å­¦ä½è®ºæ–‡: {stats['thesis_papers']}")
    print(f"æ‰«æä»¶: {stats['scanned_papers']}")
    print(f"è¯»å–å¤±è´¥: {stats['error_papers']}")
    print(f"æå–æ–¹æ³•åˆ†å¸ƒ: {stats['by_method']}")

    if keyword_freq:
        print(f"\n=== Top 20 é«˜é¢‘å…³é”®è¯ ===")
        for kw, cnt in keyword_freq[:20]:
            print(f"  {kw}: {cnt}")

    generate_readable_md(papers, stats)
    return index


def generate_readable_md(papers, stats):
    lines = []
    lines.append("# æ–‡çŒ®ç´¢å¼•åº“ v3")
    lines.append(f"\næ„å»ºæ—¶é—´: {stats['build_date']}")
    lines.append(f"æ€»è®¡: {stats['total_papers']}ç¯‡ (æœ¬åœ°{stats['local_papers']} + Zotero{stats['zotero_papers']}) | "
                 f"æœ‰æ‘˜è¦: {stats['with_abstract']}ç¯‡ | "
                 f"æœ‰å…³é”®è¯: {stats['with_keywords']}ç¯‡ | "
                 f"ä¸­æ–‡: {stats['chinese_papers']}ç¯‡ | è‹±æ–‡: {stats['english_papers']}ç¯‡")
    lines.append(f"å­¦ä½è®ºæ–‡: {stats['thesis_papers']}ç¯‡ | æ‰«æä»¶: {stats['scanned_papers']}ç¯‡\n")

    by_folder = defaultdict(list)
    for p in papers:
        by_folder[p["folder"]].append(p)

    for folder in sorted(by_folder.keys()):
        folder_papers = by_folder[folder]
        abs_count = sum(1 for p in folder_papers if p["abstract"])
        lines.append(f"\n## {folder} ({len(folder_papers)}ç¯‡, {abs_count}ç¯‡æœ‰æ‘˜è¦)\n")

        for p in folder_papers:
            name = p["filename"]
            year = f"[{p['year']}]" if p["year"] else ""
            lang = "ä¸­" if p["language"] == "zh" else "è‹±" if p["language"] == "en" else "?"
            method = p.get("extraction_method", "")
            method_tag = f" ğŸ“–{method}" if method != "standard" else ""
            thesis_tag = " ğŸ“" if p.get("is_thesis") else ""
            source_tag = " ğŸ“šZ" if p.get("source") == "zotero" else ""

            if p.get("is_scannable"):
                lines.append(f"- **{name}** {year} ({lang}) âš ï¸æ‰«æä»¶{source_tag}")
                continue

            lines.append(f"- **{name}** {year} ({lang}) {p['page_count']}é¡µ{thesis_tag}{method_tag}{source_tag}")

            if p["keywords"]:
                lines.append(f"  - å…³é”®è¯: {p['keywords'][:300]}")
            if p["abstract"]:
                abs_short = p["abstract"][:400]
                if len(p["abstract"]) > 400:
                    abs_short += "..."
                lines.append(f"  - æ‘˜è¦: {abs_short}")

    with open(OUTPUT_MD, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    md_size = os.path.getsize(OUTPUT_MD)
    print(f"å¯è¯»ç‰ˆæœ¬: {OUTPUT_MD} ({md_size/1024:.0f} KB)")


if __name__ == "__main__":
    incremental = "--incr" in sys.argv
    no_zotero = "--no-zotero" in sys.argv
    build_index(incremental=incremental, include_zotero=not no_zotero)
