#!/usr/bin/env python3
"""
æ–‡çŒ®æœç´¢å¼•æ“ v3 - jiebaåˆ†è¯ + è¯­ä¹‰æœç´¢ + æ··åˆæ’åº

ç”¨æ³•:
  python3 search_papers.py "ç ”ç©¶ä¸»é¢˜å…³é”®è¯"              # æ··åˆæœç´¢(é»˜è®¤)
  python3 search_papers.py --keyword "å…³é”®è¯1 å…³é”®è¯2"   # ä»…å…³é”®è¯æœç´¢
  python3 search_papers.py --semantic "ç ”ç©¶é—®é¢˜æè¿°"     # ä»…è¯­ä¹‰æœç´¢
  python3 search_papers.py --topic "æ ¸å¿ƒç ”ç©¶é—®é¢˜"        # ä¸»é¢˜æœç´¢ï¼ˆè‡ªåŠ¨æ‰©å±•ï¼‰
  python3 search_papers.py --folder "å­æ–‡ä»¶å¤¹å"         # æŒ‰æ–‡ä»¶å¤¹/åˆ†ç±»ç­›é€‰
  python3 search_papers.py --year-sort "å…³é”®è¯"          # æŒ‰å¹´ä»½æ’åº
  python3 search_papers.py --similar "ä½œè€…å"            # ç›¸ä¼¼è®ºæ–‡æ¨è
  python3 search_papers.py --stats                       # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
  python3 search_papers.py --top 20 "å…³é”®è¯"             # è¿”å›æ›´å¤šç»“æœ
  python3 search_papers.py "ä¸­æ–‡æŸ¥è¯¢" --also "English query"  # å¤šæŸ¥è¯¢èåˆ
"""

import json
import re
import sys
import os
import time
import jieba
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter

# è·¯å¾„ä» config.py è¯»å–ï¼Œconfig.py ä¸å­˜åœ¨æ—¶å›é€€åˆ°è„šæœ¬åŒç›®å½•
try:
    from config import INDEX_PATH, EMBEDDINGS_PATH
except ImportError:
    _BASE = Path(__file__).parent
    INDEX_PATH = _BASE / "paper_index.json"
    EMBEDDINGS_PATH = _BASE / "paper_embeddings.npz"

# ============= jieba é¢†åŸŸè¯å…¸ï¼ˆä¸build_paper_index.pyä¿æŒä¸€è‡´ï¼‰ =============
# è¯·å¡«å…¥ä½ ç ”ç©¶é¢†åŸŸçš„ä¸“ä¸šè¯æ±‡ï¼Œè®©åˆ†è¯å™¨æ­£ç¡®è¯†åˆ«è¿™äº›æœ¯è¯­ï¼ˆä¸ä¼šè¢«æ‹†æ•£ï¼‰
#
# âš¡ æ¨èï¼šè®© Claude / ChatGPT å¸®ä½ ä¸€é”®ç”Ÿæˆï¼ˆè§ README "ä½¿ç”¨AIåŠ©æ‰‹å®šåˆ¶"ç« èŠ‚ï¼‰
#    æç¤ºè¯ç¤ºä¾‹ï¼š"æˆ‘æ˜¯[ä½ çš„ç ”ç©¶æ–¹å‘]æ–¹å‘ç ”ç©¶ç”Ÿï¼Œå¸®æˆ‘ç”Ÿæˆjiebaé¢†åŸŸè¯å…¸çš„è¯æ±‡åˆ—è¡¨"
#
# æ‰‹åŠ¨å¡«å†™ç¤ºä¾‹ï¼š
#   åŒ»  å­¦ï¼š["å¿ƒè‚Œæ¢—æ­»", "åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–", "é«˜è¡€å‹", "èƒ°å²›ç´ æŠµæŠ—", "é¶å‘æ²»ç–—"]
#   è®¡ç®—æœºï¼š["ç¥ç»ç½‘ç»œ", "æ³¨æ„åŠ›æœºåˆ¶", "å¤§è¯­è¨€æ¨¡å‹", "è¿ç§»å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ "]
#   ææ–™å­¦ï¼š["çŸ³å¢¨çƒ¯", "çº³ç±³ç®¡", "è¶…å¯¼ä½“", "é’™é’›çŸ¿", "é‡‘å±æœ‰æœºæ¡†æ¶"]
#   ç»æµå­¦ï¼š["è´§å¸æ”¿ç­–", "ä¾›åº”é“¾ç®¡ç†", "ä»·æ ¼æŒ‡æ•°", "å¸‚åœºå¤±çµ", "å®è§‚ç»æµ"]
DOMAIN_WORDS = [
    # åœ¨è¿™é‡Œå¡«å…¥ä½ é¢†åŸŸçš„ä¸“ä¸šè¯æ±‡
    # "ä¸“ä¸šè¯æ±‡1", "ä¸“ä¸šè¯æ±‡2", "ä¸“ä¸šè¯æ±‡3",
]
for w in DOMAIN_WORDS:
    jieba.add_word(w)

STOPWORDS_ZH = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'å¯¹', 'åŠ', 'ç­‰', 'ä¸º', 'ä¸­',
    'ä¸Š', 'ä¸‹', 'æœ‰', 'æ— ', 'ä¸', 'ä¹Ÿ', 'åˆ', 'è¢«', 'æˆ–', 'å°†', 'æŠŠ',
    'ä»', 'åˆ°', 'ä»¥', 'ç”¨', 'å¯', 'èƒ½', 'ä¼š', 'è¦', 'å°±', 'éƒ½', 'è€Œ',
    'ä½†', 'è¿™', 'é‚£', 'å…¶', 'ä¹‹', 'æ‰€', 'è€…', 'æ­¤', 'ä¸ª', 'å·²', 'ç”±',
    'äº', 'åˆ™', 'å¹¶', 'ä¸”', 'å¦‚', 'è¿›è¡Œ', 'é€šè¿‡', 'åˆ©ç”¨', 'é‡‡ç”¨', 'åˆ†æ',
    'ç ”ç©¶', 'ç»“æœ', 'è¡¨æ˜', 'æ˜¾ç¤º', 'æå‡º', 'æé«˜', 'åŸºäº', 'æ–¹æ³•',
    'å½±å“', 'å˜åŒ–', 'æ¡ä»¶', 'ä¸åŒ', 'æƒ…å†µ', 'å…³ç³»', 'ä½œç”¨', 'å…·æœ‰',
    'ç›¸å…³', 'è¾ƒå¤§', 'è¾ƒå°', 'æ˜æ˜¾', 'ä¸»è¦', 'ä¸€å®š', 'åŒæ—¶', 'ä»¥åŠ',
    'å¤§å­¦', 'å­¦é™¢', 'å­¦æŠ¥', 'æ•™æˆ', 'åšå£«', 'ç¡•å£«', 'å¯¼å¸ˆ', 'ä½œè€…',
    'åŒ—äº¬', 'ä¸Šæµ·', 'å—äº¬', 'ä¸­å›½', 'å·¥ç¨‹', 'å­¦ä½', 'è®ºæ–‡', 'ä¸“ä¸š',
    'ç§‘å­¦', 'ç§‘å­¦é™¢', 'ç ”ç©¶æ‰€', 'ç ”ç©¶é™¢', 'å®éªŒå®¤', 'ä¸­å¿ƒ',
    'ä¸­æ–‡', 'è‹±æ–‡', 'ç¿»è¯‘', 'å…¨æ–‡', 'æ‘˜è¦', 'å…³é”®', 'å‚è€ƒ', 'æ–‡çŒ®',
}

# ä¸»é¢˜â†’åŒä¹‰è¯æ‰©å±•æ˜ å°„
# âš¡ æ¨èï¼šè®© Claude / ChatGPT å¸®ä½ ç”Ÿæˆï¼ˆè§ README "ä½¿ç”¨AIåŠ©æ‰‹å®šåˆ¶"ç« èŠ‚ï¼‰
# æ ¼å¼ï¼š"ä¸­æ–‡æ ¸å¿ƒæ¦‚å¿µ": ["English synonym1", "synonym2", "ä¸­æ–‡åŒä¹‰è¯", ...]
TOPIC_EXPANSIONS = {
    # ä»¥ä¸‹ä¸ºç¤ºä¾‹æ¡ç›®ï¼ˆå¯æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼‰
    "åŒ»å­¦å½±åƒ": ["medical imaging", "MRI", "CT scan", "X-ray", "ultrasound",
                "radiology", "image segmentation", "computer-aided diagnosis", "è¯Šæ–­"],
    "å¤§è¯­è¨€æ¨¡å‹": ["large language model", "LLM", "GPT", "BERT", "transformer",
                  "ChatGPT", "fine-tuning", "prompt engineering", "è‡ªç„¶è¯­è¨€å¤„ç†", "NLP"],
    "è¯ç‰©å‘ç°": ["drug discovery", "drug design", "molecular docking", "target",
               "é¶ç‚¹", "æŠ—ä½“", "å°åˆ†å­", "ä¸´åºŠè¯•éªŒ", "pharmacology"],
    # åœ¨æ­¤ç»§ç»­æ·»åŠ ä½ çš„æ ¸å¿ƒç ”ç©¶æ¦‚å¿µ...
    # "ä½ çš„ä¸»é¢˜": ["English synonym1", "synonym2", "ä¸­æ–‡åŒä¹‰è¯"],
}

# ============= ä¸­æ–‡â†’è‹±æ–‡æŸ¥è¯¢ç¿»è¯‘ï¼ˆè¯­ä¹‰æœç´¢ç”¨ï¼‰ =============
# å®Œæ•´æŸ¥è¯¢æ¨¡æ¿ï¼ˆä¼˜å…ˆåŒ¹é…ï¼Œæœ€ç²¾å‡†ï¼‰
# âš¡ æ¨èï¼šè®© AI å¸®ä½ ç”Ÿæˆä½ é¢†åŸŸæœ€å¸¸ç”¨çš„æŸ¥è¯¢â†’è‹±æ–‡æ‰©å±•å¯¹
# æ ¼å¼ï¼š"ä¸­æ–‡æŸ¥è¯¢çŸ­è¯­": "English keyword expansion"
_QUERY_TEMPLATES = {
    # ç¤ºä¾‹ï¼ˆåŒ»å­¦/è®¡ç®—æœºï¼Œè¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ï¼‰ï¼š
    # "é¶å‘æ²»ç–—è€è¯æ€§æœºåˆ¶": "targeted therapy drug resistance mechanism cancer",
    # "å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›è¯„ä¼°": "large language model reasoning benchmark evaluation",
    # "åŒ»å­¦å›¾åƒåˆ†å‰²æ·±åº¦å­¦ä¹ ": "medical image segmentation deep learning CNN",
}

# è¯çº§ç¿»è¯‘ï¼ˆç”¨äºè‹±æ–‡å…³é”®è¯æœç´¢é€šé“ï¼Œè®©ä¸­æ–‡æŸ¥è¯¢èƒ½åŒ¹é…è‹±æ–‡æ–‡çŒ®ï¼‰
# âš ï¸ æ­¤å­—å…¸å¿…é¡»æ ¹æ®ä½ çš„ç ”ç©¶é¢†åŸŸå®šåˆ¶ï¼Œå¦åˆ™ä¸­æ–‡æŸ¥è¯¢æ— æ³•å‘½ä¸­è‹±æ–‡æ–‡çŒ®ï¼
CN_TO_EN_QUERY = {
    # ======= é€šç”¨å­¦æœ¯åŠ¨è¯/æ–¹æ³•ï¼ˆå„å­¦ç§‘å‡é€‚ç”¨ï¼Œå»ºè®®ä¿ç•™ï¼‰=======
    "åˆ†ç±»": "classification",
    "è¯†åˆ«": "identification detection recognition",
    "å½±å“": "effect impact influence",
    "æœºåˆ¶": "mechanism pathway",
    "è¶‹åŠ¿": "trend change",
    "å“åº”": "response effect",
    "æ¨¡æ‹Ÿ": "simulation modeling",
    "é¢„æµ‹": "prediction forecast",
    "è¯„ä¼°": "assessment evaluation",
    "å½’å› ": "attribution",

    # ======= âš ï¸ è¯·åœ¨æ­¤å¡«å…¥ä½ é¢†åŸŸçš„ä¸“ç”¨è¯å¯¹ï¼ˆéå¸¸é‡è¦ï¼ï¼‰=======
    # è¯­ä¹‰æœç´¢ï¼ˆembeddingï¼‰æ— éœ€æ­¤å­—å…¸å³å¯å·¥ä½œï¼›
    # ä½† BM25 å…³é”®è¯é€šé“ä¾èµ–è¯¥è¯å…¸å°†ä¸­æ–‡è¯ç¿»è¯‘ä¸ºè‹±æ–‡å…³é”®è¯ï¼Œ
    # ç¼ºå¤±æ—¶ä¸­æ–‡æŸ¥è¯¢å°†æ— æ³•å‘½ä¸­ä»…æœ‰è‹±æ–‡æ ‡é¢˜/æ‘˜è¦çš„è®ºæ–‡ã€‚
    # è¯å¯¹è¶Šé½å…¨ï¼Œ"ä¸­æ–‡æŸ¥è¯¢ â†’ è‹±æ–‡æ–‡çŒ®"çš„å¬å›æ•ˆæœè¶Šå¥½ã€‚
    # âš¡ æ¨èï¼šè®© Claude / ChatGPT å¸®ä½ 30ç§’ç”Ÿæˆ30~50ä¸ªè¯å¯¹ï¼ˆè§ READMEï¼‰
    #
    # ç¤ºä¾‹ï¼ˆåŒ»å­¦ï¼Œå¯åˆ é™¤å¹¶æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼‰ï¼š
    # "å¿ƒè‚Œæ¢—æ­»": "myocardial infarction heart attack",
    # "é¶å‘æ²»ç–—": "targeted therapy inhibitor kinase",
    # "å…ç–«æ²»ç–—": "immunotherapy checkpoint PD-1 PD-L1",
    # "ä¸´åºŠè¯•éªŒ": "clinical trial randomized controlled",
    # "ç”Ÿç‰©æ ‡å¿—ç‰©": "biomarker marker",
    #
    # ç¤ºä¾‹ï¼ˆè®¡ç®—æœºï¼Œå¯åˆ é™¤å¹¶æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼‰ï¼š
    # "å¤§è¯­è¨€æ¨¡å‹": "large language model LLM GPT transformer",
    # "å›¾åƒè¯†åˆ«": "image recognition classification CNN",
    # "å¼ºåŒ–å­¦ä¹ ": "reinforcement learning reward policy",
}

# ============= è‹±â†’ä¸­æ ‡ç­¾æ˜ å°„ï¼ˆç»™è‹±æ–‡è®ºæ–‡ç”Ÿæˆä¸­æ–‡å…³é”®è¯ï¼‰ =============
# âš ï¸ æ­¤å­—å…¸å¿…é¡»æ ¹æ®ä½ çš„ç ”ç©¶é¢†åŸŸå®šåˆ¶ï¼å¡«å†™è¶Šå¤šï¼Œä¸­æ–‡æ£€ç´¢è‹±æ–‡è®ºæ–‡çš„è¦†ç›–ç‡è¶Šé«˜ã€‚
_EN_TO_CN_TAGS = {
    # ======= é€šç”¨å­¦æœ¯æ–¹æ³•æ ‡ç­¾ï¼ˆå„å­¦ç§‘å‡é€‚ç”¨ï¼Œå»ºè®®ä¿ç•™ï¼‰=======
    "attribution": "å½’å› åˆ†æ",
    "trend": "è¶‹åŠ¿",
    "model": "æ¨¡å‹",
    "classification": "åˆ†ç±»",

    # ======= âš ï¸ è¯·åœ¨æ­¤æ·»åŠ ä½ é¢†åŸŸçš„ä¸“ç”¨è‹±â†’ä¸­æ ‡ç­¾ï¼ˆéå¸¸é‡è¦ï¼ï¼‰=======
    # è¿™äº›æ ‡ç­¾ç”¨äºä¸ºè‹±æ–‡è®ºæ–‡è‡ªåŠ¨ç”Ÿæˆä¸­æ–‡å…³é”®è¯ï¼Œè®©è‹±æ–‡æ–‡çŒ®å¯ä»¥è¢«ä¸­æ–‡è¯æ±‡æ£€ç´¢åˆ°ã€‚
    # å¡«å†™è¶Šé½å…¨ï¼Œä¸­æ–‡å…³é”®è¯æœç´¢è‹±æ–‡è®ºæ–‡çš„è¦†ç›–ç‡è¶Šé«˜ã€‚
    # âš¡ æ¨èï¼šè®© Claude / ChatGPT å¸®ä½ ç”Ÿæˆï¼ˆè§ READMEï¼‰
    #
    # ç¤ºä¾‹ï¼ˆåŒ»å­¦ï¼Œå¯åˆ é™¤å¹¶æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼‰ï¼š
    # "myocardial infarction": "å¿ƒè‚Œæ¢—æ­»",
    # "targeted therapy": "é¶å‘æ²»ç–—",
    # "immunotherapy": "å…ç–«æ²»ç–—",
    # "clinical trial": "ä¸´åºŠè¯•éªŒ",
    # "biomarker": "ç”Ÿç‰©æ ‡å¿—ç‰©",
    # "drug resistance": "è€è¯æ€§",
    # "tumor microenvironment": "è‚¿ç˜¤å¾®ç¯å¢ƒ",
    #
    # ç¤ºä¾‹ï¼ˆè®¡ç®—æœºï¼Œå¯åˆ é™¤å¹¶æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼‰ï¼š
    # "large language model": "å¤§è¯­è¨€æ¨¡å‹",
    # "image segmentation": "å›¾åƒåˆ†å‰²",
    # "reinforcement learning": "å¼ºåŒ–å­¦ä¹ ",
    # "knowledge graph": "çŸ¥è¯†å›¾è°±",
    # "object detection": "ç›®æ ‡æ£€æµ‹",
}
_COMPOUND_TAG_RULES = [
    # âš¡ è®© AI å¸®ä½ ç”Ÿæˆï¼ˆè§ README "ä½¿ç”¨AIåŠ©æ‰‹å®šåˆ¶"ç« èŠ‚ï¼‰
    # æ ¼å¼ï¼š({"ä¸»é¢˜æ ‡ç­¾A", "ä¸»é¢˜æ ‡ç­¾B"}, "å¤åˆä¸»é¢˜æ ‡ç­¾")
    # ç¤ºä¾‹ï¼ˆåŒ»å­¦/è®¡ç®—æœºï¼Œè¯·æ›¿æ¢ï¼‰ï¼š
    # ({"æ·±åº¦å­¦ä¹ ", "åŒ»å­¦å½±åƒ"}, "åŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ "),
    # ({"å¤§è¯­è¨€æ¨¡å‹", "æ¨ç†"}, "å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›"),
]

def _generate_cn_topics(paper):
    """ä¸ºè‹±æ–‡è®ºæ–‡ç”Ÿæˆä¸­æ–‡ä¸»é¢˜æ ‡ç­¾"""
    parts = [paper.get('keywords', ''), paper.get('abstract', ''), paper.get('title_extracted', '')]
    text = ' '.join(p for p in parts if p).lower()
    # è‹¥æ ¸å¿ƒå…ƒæ•°æ®ä¸è¶³ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œè¡¥å……first_pages
    if len(text) < 100:
        fp = paper.get('first_pages', '')
        if fp:
            text += ' ' + fp[:2000].lower()
    cn = set()
    for en in sorted(_EN_TO_CN_TAGS.keys(), key=len, reverse=True):
        if en in text:
            cn.add(_EN_TO_CN_TAGS[en])
    # ä¹Ÿæ£€æŸ¥folderåç§°ä¸­çš„ä¸­æ–‡å…³é”®è¯
    folder = paper.get('folder', '')
    if folder:
        # ä»æ–‡ä»¶å¤¹åç§°ä¸­æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ_EN_TO_CN_TAGSä¸­çš„ä¸­æ–‡å€¼ + ä½ æ·»åŠ çš„é¢†åŸŸè¯ï¼‰
        for kw in set(_EN_TO_CN_TAGS.values()):
            if kw in folder:
                cn.add(kw)
    for conds, tag in _COMPOUND_TAG_RULES:
        if conds.issubset(cn):
            cn.add(tag)
    return ' '.join(cn)

def _translate_query_wordlevel(query):
    """è¯çº§ç¿»è¯‘ï¼šä¸ç”¨æ¨¡æ¿ï¼Œä»…åšæœ€é•¿åŒ¹é…è¯ç¿»è¯‘ï¼ˆç”¨äºè‹±æ–‡å…³é”®è¯æœç´¢é€šé“ï¼‰"""
    q_norm = re.sub(r'[\sï¼Œã€‚ã€ï¼šï¼›ï¼Ÿï¼\u201c\u201d\u2018\u2019ï¼ˆï¼‰()çš„ä¸å’Œå¯¹åœ¨äºä¸­]+', '', query)
    total_cn_chars = len(re.findall(r'[\u4e00-\u9fff]', q_norm))
    text = q_norm
    parts = []
    seen = set()
    translated_chars = 0
    sorted_keys = sorted(CN_TO_EN_QUERY.keys(), key=len, reverse=True)
    while text:
        matched = False
        for key in sorted_keys:
            if text.startswith(key):
                en = CN_TO_EN_QUERY[key]
                if en not in seen:
                    parts.append(en)
                    seen.add(en)
                translated_chars += len(key)
                text = text[len(key):]
                matched = True
                break
        if not matched:
            m = re.match(r'[a-zA-Z]+', text)
            if m:
                w = m.group()
                if w not in seen:
                    parts.append(w)
                    seen.add(w)
                text = text[len(w):]
            else:
                text = text[1:]
    # è‹¥ç¿»è¯‘è¦†ç›–ç‡<50%ï¼ˆå¤§é‡ä¸­æ–‡åœ°å/ä¸“æœ‰åè¯æœªç¿»è¯‘ï¼‰ï¼Œè¿”å›ç©ºä»¥é¿å…æ³›åŒ–åŒ¹é…
    if total_cn_chars > 0 and translated_chars / total_cn_chars < 0.5:
        return ''
    return ' '.join(parts)

def _translate_query(query):
    """å°†ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘ä¸ºè‹±æ–‡ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰"""
    # 1) å…ˆå°è¯•å®Œæ•´æŸ¥è¯¢æ¨¡æ¿
    q_norm = re.sub(r'[\sï¼Œã€‚ã€ï¼šï¼›ï¼Ÿï¼\u201c\u201d\u2018\u2019ï¼ˆï¼‰()]+', '', query)
    for cn, en in _QUERY_TEMPLATES.items():
        if q_norm == re.sub(r'\s+', '', cn):
            return en
    for cn, en in sorted(_QUERY_TEMPLATES.items(), key=lambda x: len(x[0]), reverse=True):
        cn_norm = re.sub(r'\s+', '', cn)
        if q_norm in cn_norm:
            return en
    # 2) æœ€é•¿åŒ¹é…ä¼˜å…ˆï¼ˆè§£å†³jiebaåˆ‡è¯ä¸åŒ¹é…CN_TO_EN_QUERYé”®çš„é—®é¢˜ï¼‰
    text = q_norm
    parts = []
    seen = set()
    sorted_keys = sorted(CN_TO_EN_QUERY.keys(), key=len, reverse=True)
    while text:
        matched = False
        for key in sorted_keys:
            if text.startswith(key):
                en = CN_TO_EN_QUERY[key]
                if en not in seen:
                    parts.append(en)
                    seen.add(en)
                text = text[len(key):]
                matched = True
                break
        if not matched:
            # æ£€æŸ¥è‹±æ–‡å­—ç¬¦
            m = re.match(r'[a-zA-Z]+', text)
            if m:
                w = m.group()
                if w not in seen:
                    parts.append(w)
                    seen.add(w)
                text = text[len(w):]
            else:
                text = text[1:]  # è·³è¿‡æ— æ³•åŒ¹é…çš„å­—ç¬¦
    return ' '.join(parts)


# ============= v3: jiebaåˆ†è¯ =============
def tokenize(text):
    """åˆ†è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰- v3ä½¿ç”¨jiebaåˆ‡è¯"""
    tokens = set()

    # è‹±æ–‡å•è¯
    en_words = re.findall(r'[a-zA-Z][a-zA-Z0-9_-]{1,}', text)
    tokens.update(w.lower() for w in en_words if len(w) >= 2)

    # ä¸­æ–‡ï¼šjiebaæœç´¢æ¨¡å¼åˆ†è¯
    zh_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
    if zh_text:
        words = jieba.cut_for_search(zh_text)
        tokens.update(w for w in words if len(w) >= 2 and w not in STOPWORDS_ZH)

    return tokens


def parse_query(query):
    """è§£ææŸ¥è¯¢ï¼šä½¿ç”¨jiebaåˆ†è¯æå–ä¸­è‹±æ–‡æŸ¥è¯¢è¯"""
    terms = []

    # è‹±æ–‡å•è¯
    en_words = re.findall(r'[a-zA-Z][a-zA-Z0-9_-]+', query)
    terms.extend(en_words)

    # ä¸­æ–‡éƒ¨åˆ†ï¼šjiebaåˆ†è¯
    zh_parts = re.findall(r'[\u4e00-\u9fff]+', query)
    for zh in zh_parts:
        words = jieba.cut(zh)
        terms.extend(w for w in words if len(w) >= 2 and w not in STOPWORDS_ZH)

    return terms


def expand_query(query_terms):
    """æ‰©å±•æŸ¥è¯¢è¯ï¼ˆæ·»åŠ åŒä¹‰è¯/ç›¸å…³è¯ï¼‰"""
    expanded = set(t.lower() for t in query_terms)
    expanded.update(query_terms)  # ä¿ç•™åŸå§‹å¤§å°å†™
    matched_topics = []

    for term in query_terms:
        term_lower = term.lower()
        for topic, synonyms in TOPIC_EXPANSIONS.items():
            synonyms_lower = [s.lower() for s in synonyms]
            if term_lower in synonyms_lower or term in topic:
                expanded.update(s.lower() for s in synonyms)
                if topic not in matched_topics:
                    matched_topics.append(topic)

    return expanded, matched_topics


# ============= å…³é”®è¯æœç´¢ =============

def score_paper_keyword(paper, query_tokens, expanded_tokens):
    """è®¡ç®—è®ºæ–‡ä¸æŸ¥è¯¢çš„å…³é”®è¯ç›¸å…³æ€§å¾—åˆ†"""
    score = 0.0

    fields = {
        "filename": 3.0,
        "keywords": 5.0,
        "abstract": 4.0,
        "title_extracted": 3.5,
        "first_pages_text": 1.0,
        "folder": 2.0,
        "zotero_meta": 2.5,  # v3: Zoteroå…ƒæ•°æ®
        "cn_topics": 3.0,    # v4: è‹±æ–‡è®ºæ–‡çš„ä¸­æ–‡æ ‡ç­¾ï¼ˆé€‚åº¦æƒé‡ï¼Œé¿å…è¿‡åº¦å‹åˆ¶ä¸­æ–‡è®ºæ–‡ï¼‰
    }

    matched_fields = []
    matched_terms = set()
    precomputed = paper.get("tokens", {})

    for field, weight in fields.items():
        # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—tokens
        if field in precomputed:
            text_tokens = set(precomputed[field])
        elif field == "first_pages_text":
            # ä»…å¯¹æ— æ‘˜è¦æ— å…³é”®è¯çš„è®ºæ–‡æ£€ç´¢å…¨æ–‡
            if paper.get("abstract") or paper.get("keywords"):
                continue
            text = paper.get(field, "")
            if not text:
                continue
            text_tokens = tokenize(text)
        else:
            text = paper.get(field, "")
            if not text:
                continue
            text_tokens = tokenize(text)

        # ç²¾ç¡®åŒ¹é…ï¼ˆåŸå§‹æŸ¥è¯¢è¯ï¼‰
        exact_matches = query_tokens & text_tokens
        if exact_matches:
            score += len(exact_matches) * weight * 2.0
            matched_fields.append(field)
            matched_terms.update(exact_matches)

        # æ‰©å±•åŒ¹é…ï¼ˆåŒä¹‰è¯ï¼‰
        expanded_matches = (expanded_tokens - query_tokens) & text_tokens
        if expanded_matches:
            score += len(expanded_matches) * weight * 0.5
            matched_terms.update(expanded_matches)

    # åŠ åˆ†é¡¹
    if paper.get("abstract"):
        if "[å…œåº•æå–]" not in paper["abstract"]:
            score *= 1.2
        else:
            score *= 1.05
    if paper.get("keywords"):
        score *= 1.1

    # åŒ¹é…å­—æ®µå¤šæ ·æ€§åŠ åˆ†
    if len(matched_fields) >= 3:
        score *= 1.3

    # æŸ¥è¯¢æ¦‚å¿µè¦†ç›–ç‡åŠ åˆ†ï¼šä¼˜å…ˆè¿”å›åŒ¹é…äº†æ‰€æœ‰æŸ¥è¯¢æ¦‚å¿µçš„è®ºæ–‡
    if len(query_tokens) >= 2:
        coverage = len(matched_terms & query_tokens) / len(query_tokens)
        if coverage >= 0.9:
            score *= 2.0  # è¦†ç›–å‡ ä¹æ‰€æœ‰æŸ¥è¯¢è¯
        elif coverage >= 0.7:
            score *= 1.5  # è¦†ç›–å¤§éƒ¨åˆ†æŸ¥è¯¢è¯
        elif coverage >= 0.5:
            score *= 1.2  # è¦†ç›–åŠæ•°æŸ¥è¯¢è¯

    return score, matched_fields, matched_terms


def keyword_search(query, papers, top_n=50, folder_filter=None, exclude_fallback=False):
    """å…³é”®è¯æœç´¢"""
    query_terms = parse_query(query)
    query_tokens = set(t.lower() for t in query_terms)
    query_tokens.update(query_terms)

    expanded_tokens, matched_topics = expand_query(query_terms)

    results = []
    for paper in papers:
        if folder_filter and folder_filter not in paper.get("folder", ""):
            continue
        if paper.get("is_scannable"):
            continue
        fname = paper.get("filename", "")
        if "å‘æ˜ä¸“åˆ©" in fname or "ä¸“è‘—" in fname:
            continue
        if exclude_fallback and paper.get("abstract", "").startswith("[å…œåº•æå–]"):
            continue

        s, matched, terms = score_paper_keyword(paper, query_tokens, expanded_tokens)
        if s > 0:
            results.append((s, matched, terms, paper))

    results.sort(key=lambda x: x[0], reverse=True)

    # å»é‡ï¼ˆå«æ¨¡ç³Šå»é‡ï¼šå»æ‰"è®ºæ–‡53-"ç­‰ç¼–å·å‰ç¼€ï¼‰
    def _norm_fn(name):
        return re.sub(r'^(?:è®ºæ–‡)?\d+[\.\-\s]+', '', name).replace(' ', '')

    seen = set()
    seen_norm = set()
    deduped = []
    for item in results:
        fn = item[3]["filename"]
        norm = _norm_fn(fn)
        if fn in seen or norm in seen_norm:
            continue
        seen.add(fn)
        seen_norm.add(norm)
        deduped.append(item)

    return deduped[:top_n], matched_topics


# ============= è¯­ä¹‰æœç´¢ =============

_embeddings_cache = {}

def load_embeddings():
    """åŠ è½½embeddingç´¢å¼•ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if 'data' in _embeddings_cache:
        return _embeddings_cache['data']

    if not EMBEDDINGS_PATH.exists():
        return None

    data = np.load(EMBEDDINGS_PATH, allow_pickle=True)
    result = {
        'embeddings': data['embeddings'],
        'filenames': list(data['filenames']),
        'model_name': str(data.get('model_name', 'unknown')),
    }
    # å»ºç«‹filenameâ†’indexæ˜ å°„
    result['filename_to_idx'] = {fn: i for i, fn in enumerate(result['filenames'])}
    _embeddings_cache['data'] = result
    return result


_model_cache = {}

def get_embedding_model(model_name):
    """åŠ è½½embeddingæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if model_name in _model_cache:
        return _model_cache[model_name]

    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer(model_name)
    _model_cache[model_name] = model
    return model


def semantic_search(query, papers, top_n=50, folder_filter=None):
    """è¯­ä¹‰æœç´¢ï¼šä¸­è‹±æ–‡å„è·‘ä¸€è½®ï¼ŒRRFåˆå¹¶æ’å"""
    emb_data = load_embeddings()
    if emb_data is None:
        return []

    model = get_embedding_model(emb_data['model_name'])

    # åŒè¯­æŸ¥è¯¢
    en_query = _translate_query(query)
    has_en = bool(en_query.strip()) and en_query != query

    queries = [query]
    if has_en:
        queries.append(en_query)

    query_embeddings = model.encode(queries, normalize_embeddings=True)

    # æ„å»ºæ–‡ä»¶åâ†’è®ºæ–‡çš„æ˜ å°„å’Œè¿‡æ»¤é›†
    fn_to_paper = {}
    skip_fns = set()
    for p in papers:
        fn = p.get('filename', '')
        fn_to_paper[fn] = p
        if p.get("is_scannable"):
            skip_fns.add(fn)
        if "å‘æ˜ä¸“åˆ©" in fn or "ä¸“è‘—" in fn:
            skip_fns.add(fn)
        if folder_filter and folder_filter not in p.get("folder", ""):
            skip_fns.add(fn)

    # å¯¹æ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆæ’å
    all_rankings = []  # list of {filename: rank}
    for qe in query_embeddings:
        sims = emb_data['embeddings'] @ qe
        ranking = {}
        rank = 0
        for idx in np.argsort(sims)[::-1]:
            fn = emb_data['filenames'][idx]
            if fn in skip_fns or fn not in fn_to_paper:
                continue
            if sims[idx] < 0.1:
                break
            rank += 1
            ranking[fn] = rank
        all_rankings.append(ranking)

    # RRFåˆå¹¶æ’åï¼ˆk=30ï¼‰
    k = 30
    rrf_scores = {}
    all_fns = set()
    for ranking in all_rankings:
        all_fns.update(ranking.keys())
    for fn in all_fns:
        score = 0.0
        for ranking in all_rankings:
            if fn in ranking:
                score += 1.0 / (k + ranking[fn])
        rrf_scores[fn] = score

    # æŒ‰RRFåˆ†æ•°æ’åºï¼Œè¾“å‡º (sim, paper) æ ¼å¼ï¼ˆsimç”¨ä¸­æ–‡æŸ¥è¯¢çš„å€¼ä¾›æ˜¾ç¤ºï¼‰
    sims_cn = emb_data['embeddings'] @ query_embeddings[0]
    fn_to_emb_idx = {fn: i for i, fn in enumerate(emb_data['filenames'])}

    sorted_fns = sorted(rrf_scores.keys(), key=lambda fn: rrf_scores[fn], reverse=True)
    results = []
    for fn in sorted_fns:
        paper = fn_to_paper.get(fn)
        if paper is None:
            continue
        emb_idx = fn_to_emb_idx.get(fn)
        sim = float(sims_cn[emb_idx]) if emb_idx is not None else 0.0
        results.append((sim, paper))
        if len(results) >= top_n:
            break

    return results


# ============= æ··åˆæœç´¢ (RRF) =============

def _is_chinese_query(query):
    """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    return bool(re.search(r'[\u4e00-\u9fff]', query))

def hybrid_search(query, papers, top_n=10, folder_filter=None, exclude_fallback=False, extra_queries=None):
    """æ··åˆæœç´¢ï¼šå…³é”®è¯ + è¯­ä¹‰ + è·¨è¯­è¨€å…³é”®è¯ï¼Œä½¿ç”¨Reciprocal Rank Fusion (RRF)

    extra_queries: é¢å¤–æŸ¥è¯¢åˆ—è¡¨ï¼ˆå¦‚è‹±æ–‡ç¿»è¯‘ï¼‰ï¼Œæ¯ä¸ªæŸ¥è¯¢ç‹¬ç«‹èµ°å…³é”®è¯+è¯­ä¹‰é€šé“ï¼Œä¸ä¸»æŸ¥è¯¢RRFèåˆã€‚
                   è¿™æ ·Claudeå¯ä»¥ç›´æ¥ä¼ å…¥ç¿»è¯‘å¥½çš„è‹±æ–‡æŸ¥è¯¢ï¼Œæ— éœ€ä¾èµ–å†…ç½®è¯å…¸ã€‚
    """
    # é€šé“1: ä¸»æŸ¥è¯¢å…³é”®è¯æœç´¢
    kw_results, matched_topics = keyword_search(
        query, papers, top_n=200, folder_filter=folder_filter, exclude_fallback=exclude_fallback
    )

    # é€šé“2: ä¸»æŸ¥è¯¢è¯­ä¹‰æœç´¢ï¼ˆå·²å†…ç½®CN/ENåŒé€šé“RRFï¼‰
    sem_results = semantic_search(query, papers, top_n=200, folder_filter=folder_filter)

    # é€šé“3: è·¨è¯­è¨€å…³é”®è¯æœç´¢ï¼ˆä¸­æ–‡æŸ¥è¯¢â†’è‹±æ–‡è¯çº§ç¿»è¯‘ï¼Œæœç´¢è‹±æ–‡è®ºæ–‡åŸå§‹å­—æ®µï¼‰
    en_kw_results = []
    if _is_chinese_query(query):
        en_query = _translate_query_wordlevel(query)
        if en_query and en_query.strip():
            en_kw_results, _ = keyword_search(
                en_query, papers, top_n=200, folder_filter=folder_filter, exclude_fallback=exclude_fallback
            )

    # RRFèåˆ
    k = 60  # RRFå¸¸æ•°
    paper_scores = defaultdict(float)
    paper_data = {}  # filename â†’ (matched_fields, matched_terms, paper)

    # é€šé“1: ä¸»æŸ¥è¯¢å…³é”®è¯æ’åè´¡çŒ®
    for rank, (score, matched, terms, paper) in enumerate(kw_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k + rank + 1)
        paper_data[fn] = (matched, terms, paper, score)

    # é€šé“2: ä¸»æŸ¥è¯¢è¯­ä¹‰æ’åè´¡çŒ®
    for rank, (sim, paper) in enumerate(sem_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k + rank + 1)
        if fn not in paper_data:
            paper_data[fn] = (["semantic"], set(), paper, 0)

    # é€šé“3: å†…ç½®ç¿»è¯‘è‹±æ–‡å…³é”®è¯ï¼ˆk=100ï¼Œä½œä¸ºè½»å¾®æå‡ï¼‰
    k_en = 100
    for rank, (score, matched, terms, paper) in enumerate(en_kw_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k_en + rank + 1)
        if fn not in paper_data:
            paper_data[fn] = (matched, terms, paper, score)

    # é¢å¤–æŸ¥è¯¢é€šé“ï¼ˆç”±è°ƒç”¨æ–¹æä¾›ï¼Œå¦‚Claudeç¿»è¯‘çš„è‹±æ–‡æŸ¥è¯¢ï¼‰
    extra_sem_results_all = []
    if extra_queries:
        for eq in extra_queries:
            eq = eq.strip()
            if not eq:
                continue
            # é¢å¤–æŸ¥è¯¢çš„å…³é”®è¯é€šé“
            eq_kw, _ = keyword_search(eq, papers, top_n=200, folder_filter=folder_filter,
                                       exclude_fallback=exclude_fallback)
            for rank, (score, matched, terms, paper) in enumerate(eq_kw):
                fn = paper["filename"]
                paper_scores[fn] += 1.0 / (k + rank + 1)
                if fn not in paper_data:
                    paper_data[fn] = (matched, terms, paper, score)

            # é¢å¤–æŸ¥è¯¢çš„è¯­ä¹‰é€šé“
            eq_sem = semantic_search(eq, papers, top_n=200, folder_filter=folder_filter)
            extra_sem_results_all.extend(eq_sem)
            for rank, (sim, paper) in enumerate(eq_sem):
                fn = paper["filename"]
                paper_scores[fn] += 1.0 / (k + rank + 1)
                if fn not in paper_data:
                    paper_data[fn] = (["semantic"], set(), paper, 0)

    # æŒ‰RRFåˆ†æ•°æ’åº
    sorted_fns = sorted(paper_scores.keys(), key=lambda fn: paper_scores[fn], reverse=True)

    results = []
    all_sem = sem_results + extra_sem_results_all
    for fn in sorted_fns[:top_n]:
        matched, terms, paper, kw_score = paper_data[fn]
        rrf_score = paper_scores[fn]

        # è·å–è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå–æ‰€æœ‰è¯­ä¹‰é€šé“çš„æœ€é«˜å€¼ï¼‰
        sem_sim = 0.0
        for sim, p in all_sem:
            if p["filename"] == fn:
                sem_sim = max(sem_sim, sim)

        # åœ¨ä¸»æŸ¥è¯¢å…³é”®è¯æœç´¢ä¸­çš„æ’å
        kw_rank = -1
        for i, (_, _, _, p) in enumerate(kw_results):
            if p["filename"] == fn:
                kw_rank = i + 1
                break

        # åœ¨ä¸»æŸ¥è¯¢è¯­ä¹‰æœç´¢ä¸­çš„æ’å
        sem_rank = -1
        for i, (_, p) in enumerate(sem_results):
            if p["filename"] == fn:
                sem_rank = i + 1
                break

        results.append({
            "paper": paper,
            "rrf_score": rrf_score,
            "kw_score": kw_score,
            "sem_sim": sem_sim,
            "kw_rank": kw_rank,
            "sem_rank": sem_rank,
            "matched_fields": matched,
            "matched_terms": terms,
        })

    return results, matched_topics


# ============= ç›¸ä¼¼è®ºæ–‡æ¨è =============

def find_similar(query_name, papers, top_n=10):
    """ç›¸ä¼¼è®ºæ–‡æ¨è"""
    target = None
    for p in papers:
        if query_name.lower() in p["filename"].lower():
            target = p
            break
    if not target:
        return [], [], query_name

    # æ„å»ºæœç´¢æŸ¥è¯¢
    search_text = f"{target.get('keywords', '')} {target.get('abstract', '')[:200]}"
    if not search_text.strip():
        search_text = target.get("first_pages_text", "")[:500]

    # ç”¨æ··åˆæœç´¢
    results, topics = hybrid_search(search_text, papers, top_n=top_n + 1)

    # æ’é™¤è‡ªèº«
    results = [r for r in results if r["paper"]["filename"] != target["filename"]][:top_n]

    return results, topics, target["filename"]


# ============= æ ¼å¼åŒ–è¾“å‡º =============

def format_results(results, query, matched_topics=None, similar_source=None, search_mode="hybrid"):
    """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
    lines = []

    if similar_source:
        lines.append(f"## ä¸ \"{similar_source}\" ç›¸ä¼¼çš„è®ºæ–‡")
    else:
        lines.append(f"## æœç´¢: \"{query}\" [{search_mode}]")

    if matched_topics:
        lines.append(f"æ‰©å±•ä¸»é¢˜: {', '.join(matched_topics)}")

    lines.append(f"æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡\n")

    for rank, item in enumerate(results, 1):
        if isinstance(item, dict):
            # æ··åˆæœç´¢ç»“æœ
            p = item["paper"]
            rrf = item["rrf_score"]
            kw_score = item["kw_score"]
            sem_sim = item["sem_sim"]
            kw_rank = item["kw_rank"]
            sem_rank = item["sem_rank"]
            matched = item["matched_fields"]
            terms = item["matched_terms"]

            score_parts = []
            if kw_rank > 0:
                score_parts.append(f"å…³é”®è¯#{kw_rank}")
            if sem_rank > 0:
                score_parts.append(f"è¯­ä¹‰#{sem_rank}({sem_sim:.2f})")
            score_info = " | ".join(score_parts) if score_parts else ""

            lines.append(f"### [{rank}] RRF: {rrf:.4f}  {score_info}")
        else:
            # æ—§æ ¼å¼å…¼å®¹
            score, matched, terms, p = item
            lines.append(f"### [{rank}] ç›¸å…³åº¦: {score:.1f}")

        lang = "ä¸­" if p["language"] == "zh" else "è‹±"
        year = p["year"] or "?"
        pages = p["page_count"]
        thesis = " ğŸ“" if p.get("is_thesis") else ""
        source = " ğŸ“šZ" if p.get("source") == "zotero" else ""

        lines.append(f"**{p['filename']}** ({lang}, {year}, {pages}é¡µ{thesis}{source})")
        lines.append(f"æ–‡ä»¶å¤¹: {p['folder']}")

        if matched:
            lines.append(f"åŒ¹é…å­—æ®µ: {', '.join(matched)}")
        if terms:
            display_terms = sorted(terms, key=len, reverse=True)[:10]
            lines.append(f"åŒ¹é…è¯: {', '.join(display_terms)}")

        # æ˜¾ç¤ºZoteroå…ƒæ•°æ®
        if p.get("zotero_title"):
            lines.append(f"Zoteroæ ‡é¢˜: {p['zotero_title']}")
        if p.get("zotero_authors"):
            authors = ", ".join(p["zotero_authors"][:3])
            if len(p["zotero_authors"]) > 3:
                authors += f" ç­‰({len(p['zotero_authors'])}äºº)"
            lines.append(f"ä½œè€…: {authors}")

        if p["keywords"]:
            lines.append(f"å…³é”®è¯: {p['keywords'][:300]}")
        if p["abstract"]:
            abs_text = p["abstract"][:500]
            if len(p["abstract"]) > 500:
                abs_text += "..."
            lines.append(f"æ‘˜è¦: {abs_text}")

        lines.append(f"è·¯å¾„: {p['path']}")
        lines.append("")

    return "\n".join(lines)


def show_stats():
    """æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡"""
    index = load_index()
    stats = index["stats"]

    print("=== æ–‡çŒ®ç´¢å¼•ç»Ÿè®¡ v3 ===")
    for k, v in stats.items():
        if k == "top_keywords":
            continue
        if k == "by_method":
            print(f"  æå–æ–¹æ³•åˆ†å¸ƒ:")
            for mk, mv in v.items():
                print(f"    {mk}: {mv}")
        else:
            print(f"  {k}: {v}")

    # æŒ‰æ–‡ä»¶å¤¹ç»Ÿè®¡
    by_folder = defaultdict(lambda: {"total": 0, "with_abs": 0, "thesis": 0, "local": 0, "zotero": 0})
    for p in index["papers"]:
        f = p["folder"]
        by_folder[f]["total"] += 1
        if p["abstract"]:
            by_folder[f]["with_abs"] += 1
        if p.get("is_thesis"):
            by_folder[f]["thesis"] += 1
        if p.get("source") == "zotero":
            by_folder[f]["zotero"] += 1
        else:
            by_folder[f]["local"] += 1

    print("\n=== æŒ‰æ–‡ä»¶å¤¹/åˆ†ç±»åˆ†å¸ƒ ===")
    for folder in sorted(by_folder.keys()):
        d = by_folder[folder]
        src = f"æœ¬åœ°{d['local']}" + (f"+Z{d['zotero']}" if d['zotero'] else "")
        print(f"  {folder}: {d['total']}ç¯‡ ({src}, æ‘˜è¦{d['with_abs']})")

    # EmbeddingçŠ¶æ€
    if EMBEDDINGS_PATH.exists():
        data = np.load(EMBEDDINGS_PATH, allow_pickle=True)
        print(f"\n=== Embeddingç´¢å¼• ===")
        print(f"  è®ºæ–‡æ•°: {len(data['filenames'])}")
        print(f"  å‘é‡ç»´åº¦: {data['embeddings'].shape[1]}")
        print(f"  æ¨¡å‹: {data.get('model_name', 'unknown')}")
        print(f"  æ–‡ä»¶å¤§å°: {EMBEDDINGS_PATH.stat().st_size/1024/1024:.1f} MB")
    else:
        print(f"\nâš ï¸ æ— Embeddingç´¢å¼• (è¿è¡Œ python3 build_embeddings.py åˆ›å»º)")

    # é«˜é¢‘å…³é”®è¯
    top_kw = stats.get("top_keywords", [])
    if top_kw:
        print(f"\n=== Top 30 é«˜é¢‘å…³é”®è¯ ===")
        for item in top_kw[:30]:
            print(f"  {item['keyword']}: {item['count']}")


def load_index():
    with open(INDEX_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    # ä¸ºè‹±æ–‡è®ºæ–‡åŠ¨æ€ç”Ÿæˆcn_topicså­—æ®µ
    for p in data["papers"]:
        if p.get("language") == "en" and not p.get("cn_topics"):
            p["cn_topics"] = _generate_cn_topics(p)
    return data


def main():
    args = sys.argv[1:]

    if not args:
        print(__doc__)
        return

    if "--stats" in args:
        show_stats()
        return

    top_n = 10
    folder_filter = None
    topic_mode = False
    year_sort = False
    similar_mode = False
    exclude_fallback = False
    search_mode = "hybrid"  # hybrid, keyword, semantic
    query_parts = []
    also_queries = []  # é¢å¤–æŸ¥è¯¢ï¼ˆå¤šæŸ¥è¯¢RRFèåˆï¼‰

    i = 0
    while i < len(args):
        if args[i] == "--top" and i + 1 < len(args):
            top_n = int(args[i + 1])
            i += 2
        elif args[i] == "--folder" and i + 1 < len(args):
            folder_filter = args[i + 1]
            i += 2
        elif args[i] == "--topic":
            topic_mode = True
            i += 1
        elif args[i] == "--year-sort":
            year_sort = True
            i += 1
        elif args[i] == "--similar":
            similar_mode = True
            i += 1
        elif args[i] == "--no-fallback":
            exclude_fallback = True
            i += 1
        elif args[i] == "--keyword":
            search_mode = "keyword"
            i += 1
        elif args[i] == "--semantic":
            search_mode = "semantic"
            i += 1
        elif args[i] == "--hybrid":
            search_mode = "hybrid"
            i += 1
        elif args[i] == "--also" and i + 1 < len(args):
            also_queries.append(args[i + 1])
            i += 2
        else:
            query_parts.append(args[i])
            i += 1

    query = " ".join(query_parts)
    if not query:
        print("è¯·æä¾›æœç´¢å…³é”®è¯")
        return

    # åŠ è½½ç´¢å¼•
    index = load_index()
    papers = index["papers"]

    if similar_mode:
        results, topics, source = find_similar(query, papers, top_n=top_n)
        output = format_results(results, query, matched_topics=topics, similar_source=source)
        print(output)
        return

    # æ ¹æ®æ¨¡å¼æœç´¢
    if search_mode == "keyword":
        results, topics = keyword_search(query, papers, top_n=top_n,
                                         folder_filter=folder_filter,
                                         exclude_fallback=exclude_fallback)
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted = []
        for score, matched, terms, paper in results:
            formatted.append({
                "paper": paper,
                "rrf_score": score,
                "kw_score": score,
                "sem_sim": 0,
                "kw_rank": 0,
                "sem_rank": 0,
                "matched_fields": matched,
                "matched_terms": terms,
            })
        output = format_results(formatted, query, matched_topics=topics, search_mode="keyword")

    elif search_mode == "semantic":
        if not EMBEDDINGS_PATH.exists():
            print("âš ï¸ æ— Embeddingç´¢å¼•ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
            print("  è¯·å…ˆè¿è¡Œ: python3 build_embeddings.py")
            search_mode = "keyword"
            results, topics = keyword_search(query, papers, top_n=top_n,
                                             folder_filter=folder_filter)
            formatted = []
            for score, matched, terms, paper in results:
                formatted.append({
                    "paper": paper,
                    "rrf_score": score,
                    "kw_score": score,
                    "sem_sim": 0,
                    "kw_rank": 0,
                    "sem_rank": 0,
                    "matched_fields": matched,
                    "matched_terms": terms,
                })
            output = format_results(formatted, query, matched_topics=topics, search_mode="keyword(fallback)")
        else:
            sem_results = semantic_search(query, papers, top_n=top_n, folder_filter=folder_filter)
            formatted = []
            for rank, (sim, paper) in enumerate(sem_results):
                formatted.append({
                    "paper": paper,
                    "rrf_score": sim,
                    "kw_score": 0,
                    "sem_sim": sim,
                    "kw_rank": 0,
                    "sem_rank": rank + 1,
                    "matched_fields": ["semantic"],
                    "matched_terms": set(),
                })
            output = format_results(formatted, query, search_mode="semantic")

    else:  # hybrid
        if not EMBEDDINGS_PATH.exists():
            # æ— embeddingï¼Œå›é€€åˆ°çº¯å…³é”®è¯æœç´¢
            results, topics = keyword_search(query, papers, top_n=top_n,
                                             folder_filter=folder_filter,
                                             exclude_fallback=exclude_fallback)
            formatted = []
            for score, matched, terms, paper in results:
                formatted.append({
                    "paper": paper,
                    "rrf_score": score,
                    "kw_score": score,
                    "sem_sim": 0,
                    "kw_rank": 0,
                    "sem_rank": 0,
                    "matched_fields": matched,
                    "matched_terms": terms,
                })
            output = format_results(formatted, query, matched_topics=topics, search_mode="keyword(no embedding)")
        else:
            results, topics = hybrid_search(query, papers, top_n=top_n,
                                            folder_filter=folder_filter,
                                            exclude_fallback=exclude_fallback,
                                            extra_queries=also_queries if also_queries else None)
            mode_label = f"hybrid+{len(also_queries)}q" if also_queries else "hybrid"
            output = format_results(results, query, matched_topics=topics, search_mode=mode_label)

    if year_sort and search_mode != "semantic":
        # å¹´ä»½æ’åºæ¨¡å¼ä¸‹é‡æ’
        print("(ç»“æœå·²æŒ‰ç›¸å…³åº¦æ’åºï¼Œæ·»åŠ --year-sortä»…åœ¨keywordæ¨¡å¼ä¸‹æŒ‰å¹´ä»½æ’åº)")

    print(output)


if __name__ == "__main__":
    main()
