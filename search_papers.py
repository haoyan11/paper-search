#!/usr/bin/env python3
"""
æ–‡çŒ®æœç´¢å¼•æ“ v3 - jiebaåˆ†è¯ + è¯­ä¹‰æœç´¢ + æ··åˆæ’åº

ç”¨æ³•:
  python3 search_papers.py "ç‰©å€™å¯¹å¾„æµçš„å½±å“"              # æ··åˆæœç´¢(é»˜è®¤)
  python3 search_papers.py --keyword "ç‰©å€™ æ°´æ–‡ SWAT"      # ä»…å…³é”®è¯æœç´¢
  python3 search_papers.py --semantic "æ°”å€™å˜åŒ–å¦‚ä½•å½±å“æ°´å¾ªç¯"  # ä»…è¯­ä¹‰æœç´¢
  python3 search_papers.py --topic "æ¤è¢«ç‰©å€™å¯¹å¾„æµçš„å½±å“æœºåˆ¶"  # ä¸»é¢˜æœç´¢ï¼ˆè‡ªåŠ¨æ‰©å±•ï¼‰
  python3 search_papers.py --folder "ç‰©å€™å¯¹æ°´å½±å“"          # æŒ‰æ–‡ä»¶å¤¹/åˆ†ç±»ç­›é€‰
  python3 search_papers.py --year-sort "climate NDVI"      # æŒ‰å¹´ä»½æ’åº
  python3 search_papers.py --similar "éŸ©æ·‘é¢–"              # ç›¸ä¼¼è®ºæ–‡æ¨è
  python3 search_papers.py --stats                         # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
  python3 search_papers.py --top 20 "SWAT model"          # è¿”å›æ›´å¤šç»“æœ
  python3 search_papers.py "ç‰©å€™ æ°´æ–‡" --also "phenology hydrology streamflow"  # å¤šæŸ¥è¯¢èåˆ
"""

import json
import re
import sys
import os
import time
import jieba
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter

# è·¯å¾„ä» config.py è¯»å–ï¼Œconfig.py ä¸å­˜åœ¨æ—¶å›é€€åˆ°è„šæœ¬åŒç›®å½•
try:
    from config import INDEX_PATH, EMBEDDINGS_PATH
except ImportError:
    _BASE = Path(__file__).parent
    INDEX_PATH = _BASE / "paper_index.json"
    EMBEDDINGS_PATH = _BASE / "paper_embeddings.npz"

# ============= jieba é¢†åŸŸè¯å…¸ï¼ˆä¸build_paper_index.pyä¿æŒä¸€è‡´ï¼‰ =============
DOMAIN_WORDS = [
    "ç‰©å€™", "ç‰©å€™æœŸ", "ç‰©å€™å˜åŒ–", "è¿”é’æœŸ", "æ¯é»„æœŸ", "ç”Ÿé•¿å­£",
    "è’¸æ•£", "è’¸æ•£å‘", "è’¸è…¾", "æ½œåœ¨è’¸æ•£", "å®é™…è’¸æ•£",
    "å¾„æµ", "äº§æµ", "åŸºæµ", "æ¯æ°´", "æ´ªæ°´", "æ´ªå³°",
    "å¾„æµé‡", "å¾„æµæ·±", "å¾„æµç³»æ•°", "å¤©ç„¶å¾„æµ",
    "æ¤è¢«è¦†ç›–", "æ¤è¢«æ¢å¤", "æ¤è¢«æŒ‡æ•°", "æ¤è¢«åŠ¨æ€",
    "é¥æ„Ÿåæ¼”", "é¥æ„Ÿç›‘æµ‹", "é¥æ„Ÿæ•°æ®",
    "æ°´æºæ¶µå…»", "æ°´æºæ¶µå…»é‡", "æ—å† æˆªç•™",
    "ç”Ÿæ€æµé‡", "ç”Ÿæ€éœ€æ°´", "ç”Ÿæ€åŸºæµ", "ç¯å¢ƒæµé‡",
    "é»„åœŸé«˜åŸ", "è¥¿è¾½æ²³", "æµ·æ²³æµåŸŸ", "é»„æ²³æµåŸŸ", "é•¿æ±ŸæµåŸŸ",
    "æ°”å€™å˜åŒ–", "æ°”å€™å˜æš–", "å…¨çƒå˜æš–", "æç«¯æ°”å€™",
    "é€€è€•è¿˜æ—", "é€€è€•è¿˜è‰", "æ°´åœŸä¿æŒ",
    "æ·¤åœ°å", "é±¼é³å‘", "æ°´å¹³é˜¶", "æ¢¯ç”°",
    "ç¢³å¾ªç¯", "ç¢³æ±‡", "ç¢³å‚¨é‡", "å‡€åˆçº§ç”Ÿäº§åŠ›",
    "å½’å› åˆ†æ", "å¼¹æ€§ç³»æ•°", "æ•æ„Ÿæ€§åˆ†æ",
    "ç»“æ„æ–¹ç¨‹", "é€šå¾„åˆ†æ", "å› æœåˆ†æ",
    "éšæœºæ£®æ—", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ",
    "ç”Ÿæ€æ°´æ–‡", "æ°´æ–‡æ¨¡å‹", "æ°´æ–‡æ•ˆåº”",
    "å¹²æ—±æŒ‡æ•°", "å¹²æ—±äº‹ä»¶", "å¹²æ—±èƒè¿«",
    "æ—¶ç©ºå˜åŒ–", "æ—¶ç©ºæ ¼å±€", "æ—¶ç©ºåˆ†å¸ƒ",
    "æ°´æ–‡æ°”å€™", "æ°´çƒ­è€¦åˆ", "æ°´é‡å¹³è¡¡",
]
for w in DOMAIN_WORDS:
    jieba.add_word(w)

STOPWORDS_ZH = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'å¯¹', 'åŠ', 'ç­‰', 'ä¸º', 'ä¸­',
    'ä¸Š', 'ä¸‹', 'æœ‰', 'æ— ', 'ä¸', 'ä¹Ÿ', 'åˆ', 'è¢«', 'æˆ–', 'å°†', 'æŠŠ',
    'ä»', 'åˆ°', 'ä»¥', 'ç”¨', 'å¯', 'èƒ½', 'ä¼š', 'è¦', 'å°±', 'éƒ½', 'è€Œ',
    'ä½†', 'è¿™', 'é‚£', 'å…¶', 'ä¹‹', 'æ‰€', 'è€…', 'æ­¤', 'ä¸ª', 'å·²', 'ç”±',
    'äº', 'åˆ™', 'å¹¶', 'ä¸”', 'å¦‚', 'è¿›è¡Œ', 'é€šè¿‡', 'åˆ©ç”¨', 'é‡‡ç”¨', 'åˆ†æ',
    'ç ”ç©¶', 'ç»“æœ', 'è¡¨æ˜', 'æ˜¾ç¤º', 'æå‡º', 'æé«˜', 'åŸºäº', 'æ–¹æ³•',
    'å½±å“', 'å˜åŒ–', 'æ¡ä»¶', 'ä¸åŒ', 'æƒ…å†µ', 'å…³ç³»', 'ä½œç”¨', 'å…·æœ‰',
    'ç›¸å…³', 'è¾ƒå¤§', 'è¾ƒå°', 'æ˜æ˜¾', 'ä¸»è¦', 'ä¸€å®š', 'åŒæ—¶', 'ä»¥åŠ',
    'å¤§å­¦', 'å­¦é™¢', 'å­¦æŠ¥', 'æ•™æˆ', 'åšå£«', 'ç¡•å£«', 'å¯¼å¸ˆ', 'ä½œè€…',
    'åŒ—äº¬', 'ä¸Šæµ·', 'å—äº¬', 'ä¸­å›½', 'å·¥ç¨‹', 'å­¦ä½', 'è®ºæ–‡', 'ä¸“ä¸š',
    'ç§‘å­¦', 'ç§‘å­¦é™¢', 'ç ”ç©¶æ‰€', 'ç ”ç©¶é™¢', 'å®éªŒå®¤', 'ä¸­å¿ƒ',
    'ä¸­æ–‡', 'è‹±æ–‡', 'ç¿»è¯‘', 'å…¨æ–‡', 'æ‘˜è¦', 'å…³é”®', 'å‚è€ƒ', 'æ–‡çŒ®',
}

# ä¸»é¢˜â†’åŒä¹‰è¯æ‰©å±•æ˜ å°„
TOPIC_EXPANSIONS = {
    "ç‰©å€™": ["phenology", "phenological", "ç‰©å€™", "SOS", "EOS", "LOS", "è¿”é’", "æ¯é»„", "ç”Ÿé•¿å­£",
             "green-up", "greenup", "senescence", "growing season", "Double Logistic", "TIMESAT",
             "spring onset", "autumn", "dormancy", "leaf onset", "leaf-out", "è¿”é’æœŸ", "æ¯é»„æœŸ",
             "ç‰©å€™æœŸ", "ç‰©å€™å˜åŒ–"],
    "æ°´æ–‡": ["hydrological", "hydrology", "æ°´æ–‡", "å¾„æµ", "runoff", "streamflow", "discharge",
             "äº§æµ", "äº§æ°´", "water yield", "baseflow", "åŸºæµ", "æ´ªæ°´", "flood", "æ¯æ°´",
             "æ°´å¾ªç¯", "water cycle", "æ°´é‡å¹³è¡¡", "water balance", "æ°´æ–‡æ•ˆåº”", "æ°´æ–‡æ¨¡å‹"],
    "è’¸æ•£": ["evapotranspiration", "ET", "è’¸æ•£", "è’¸å‘", "PET", "AET", "Penman", "PML",
             "MODIS ET", "MOD16", "transpiration", "è’¸è…¾", "GLEAM", "è’¸æ•£å‘"],
    "SWAT": ["SWAT", "swat", "SWAT+", "soil water assessment", "HRU", "SWAT-CUP",
             "SUFI-2", "å­æµåŸŸ"],
    "é¥æ„Ÿ": ["remote sensing", "é¥æ„Ÿ", "NDVI", "LAI", "EVI", "MODIS", "Landsat", "sentinel",
             "å«æ˜Ÿ", "satellite", "GEE", "Google Earth Engine", "é¥æ„Ÿåæ¼”"],
    "æ°”å€™å˜åŒ–": ["climate change", "æ°”å€™å˜åŒ–", "æ°”å€™å˜æš–", "global warming", "temperature",
                "precipitation", "é™æ°´", "æ°”æ¸©", "CMIP", "RCP", "SSP", "å˜æš–",
                "å¹²æ—±", "drought", "æç«¯æ°”å€™", "extreme climate"],
    "æ°”å€™å› å­": ["climate factor", "climate variable", "climatic variable", "æ°”å€™å› å­",
                "temperature", "precipitation", "humidity", "vapor pressure deficit", "VPD",
                "radiation", "solar radiation", "wind", "wind speed", "photoperiod",
                "æ¸©åº¦", "é™æ°´", "æ¹¿åº¦", "è¾å°„", "é£é€Ÿ", "æ°”æ¸©", "æ°”å€™é©±åŠ¨", "climate driver"],
    "æ°´æ–‡æ•ˆåº”": ["hydrological effect", "hydrological impact", "æ°´æ–‡æ•ˆåº”", "æ°´æ–‡å½±å“",
                "evapotranspiration", "è’¸æ•£å‘", "runoff", "å¾„æµ", "streamflow",
                "water yield", "äº§æ°´é‡", "water cycle", "æ°´å¾ªç¯"],
    "æ¤è¢«": ["vegetation", "æ¤è¢«", "forest", "è‰åœ°", "grassland", "çŒæœ¨", "shrub", "è¦†è¢«",
            "land cover", "land use", "LUCC", "greening", "ç»¿åŒ–", "æ¤è¢«è¦†ç›–",
            "NDVI", "æ¤è¢«æ¢å¤", "é€€è€•è¿˜æ—", "é€ æ—", "afforestation"],
    "å½’å› ": ["attribution", "å½’å› ", "è´¡çŒ®", "contribution", "é©±åŠ¨", "driver", "å½±å“å› ç´ ",
            "sensitivity", "elasticity", "å¼¹æ€§", "Budyko", "åˆ†ç¦»", "decomposition", "å½’å› åˆ†æ"],
    "åœŸå£¤": ["soil", "åœŸå£¤", "soil moisture", "åœŸå£¤æ°´", "infiltration", "å…¥æ¸—", "åœŸå£¤ä¾µèš€",
            "soil erosion", "RUSLE", "USLE", "åœŸå£¤æœ‰æœºç¢³", "SOC"],
    "ç”Ÿæ€æµé‡": ["ecological flow", "ç”Ÿæ€æµé‡", "ç¯å¢ƒæµé‡", "e-flow", "minimum flow",
                "æœ€å°ç”Ÿæ€éœ€æ°´", "ç”Ÿæ€éœ€æ°´", "ç”Ÿæ€åŸºæµ"],
    "ç¢³å¾ªç¯": ["carbon", "ç¢³", "NPP", "GPP", "NEP", "å›ºç¢³", "ç¢³æ±‡", "carbon sequestration",
              "Biome-BGC", "carbon cycle", "ç¢³å‚¨é‡", "ç¢³å¾ªç¯", "å‡€åˆçº§ç”Ÿäº§åŠ›"],
    "æ¨¡å‹": ["model", "æ¨¡å‹", "simulation", "æ¨¡æ‹Ÿ", "calibration", "ç‡å®š", "validation", "éªŒè¯",
            "NSE", "RMSE", "å‚æ•°", "ä¸ç¡®å®šæ€§", "uncertainty"],
    "ç”Ÿæ€æ°´æ–‡": ["ecohydrology", "eco-hydrology", "ç”Ÿæ€æ°´æ–‡", "æ°´æ–‡ç”Ÿæ€",
               "vegetation-hydrology", "æ¤è¢«æ°´æ–‡", "æ°´æ–‡æ•ˆåº”"],
    "æµ·æ²³": ["æµ·æ²³", "Hai River", "Haihe", "ååŒ—", "North China"],
    "é»„åœŸé«˜åŸ": ["é»„åœŸé«˜åŸ", "Loess Plateau", "é»„åœŸ", "loess", "é»„æ²³", "Yellow River"],
    "éšæœºæ£®æ—": ["random forest", "éšæœºæ£®æ—", "machine learning", "æœºå™¨å­¦ä¹ ", "SHAP",
                "XGBoost", "neural network", "deep learning", "æ·±åº¦å­¦ä¹ ", "CNN", "LSTM"],
    "Budyko": ["Budyko", "æ°´çƒ­è€¦åˆ", "å¹²ç‡¥åº¦", "aridity index", "è’¸æ•£æ¯”",
              "evaporative index", "æ°´é‡å¹³è¡¡", "å¼¹æ€§ç³»æ•°"],
    "å¹²æ—±": ["drought", "å¹²æ—±", "SPEI", "SPI", "PDSI", "å¹²æ—±æŒ‡æ•°", "å¹²æ—±äº‹ä»¶",
            "å¹²æ—±èƒè¿«", "æ°´åˆ†èƒè¿«", "water stress"],
}

# ============= ä¸­æ–‡â†’è‹±æ–‡æŸ¥è¯¢ç¿»è¯‘ï¼ˆè¯­ä¹‰æœç´¢ç”¨ï¼‰ =============
# å®Œæ•´æŸ¥è¯¢æ¨¡æ¿ï¼ˆä¼˜å…ˆåŒ¹é…ï¼Œæœ€ç²¾å‡†ï¼‰
_QUERY_TEMPLATES = {
    "ç‰©å€™å˜åŒ–çš„æ°´æ–‡æ•ˆåº”": "phenology growing season evapotranspiration hydrology",
    "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”": "phenology growing season evapotranspiration hydrology",
    "æ°”å€™å› å­å¯¹ç‰©å€™çš„å½±å“": "climate temperature precipitation phenology",
    "æ°”å€™å› å­ç‰©å€™å½±å“": "climate temperature precipitation phenology",
    "æ°”å€™å˜åŒ–ç”Ÿé•¿å­£è’¸æ•£å‘": "climate change growing season evapotranspiration",
    "ç”Ÿé•¿å­£å»¶é•¿è’¸æ•£å‘å¢åŠ ": "growing season lengthening evapotranspiration increase",
    "ç‰©å€™å¯¹æ°´æ–‡çš„å½±å“": "phenology evapotranspiration streamflow hydrology",
    "ç‰©å€™å¯¹å¾„æµçš„å½±å“": "phenology runoff streamflow discharge",
    "æ¤è¢«ç‰©å€™å˜åŒ–": "vegetation phenology change",
    "æ˜¥å­£ç‰©å€™æå‰": "spring phenology advance earlier greenup",
    "ç§‹å­£ç‰©å€™æ¨è¿Ÿ": "autumn phenology delay senescence",
    "æ°”å€™å˜åŒ–å¯¹æ¤è¢«çš„å½±å“": "climate change vegetation response",
    "ç‰©å€™é¥æ„Ÿåæ¼”": "phenology remote sensing satellite NDVI",
    "è’¸æ•£å‘å½’å› åˆ†æ": "evapotranspiration attribution climate vegetation",
    "æ°´æ–‡æ¨¡å‹æ¨¡æ‹Ÿ": "hydrological model simulation",
    "æ¤è¢«å˜åŒ–æ°´æ–‡å“åº”": "vegetation change hydrological response runoff",
    "è€ƒè™‘ç‰©å€™çš„æ°´æ–‡æ¨¡å‹": "phenology hydrological model SWAT ecohydrology vegetation",
    "æ°´æ–‡æ¨¡å‹ä¸­çš„ç‰©å€™å‚æ•°": "phenology parameter hydrological model growing season LAI",
    "å¤©ç„¶æ¤è¢«ä¸äººå·¥æ¤è¢«åˆ’åˆ†": "natural vegetation planted forest classification mapping",
    "å¤©ç„¶æ—äººå·¥æ—è¯†åˆ«": "natural forest planted forest classification mapping distinguish",
}

# è¯çº§ç¿»è¯‘ï¼ˆä¸°å¯ŒåŒä¹‰è¯ç‰ˆï¼Œç”¨äºè‹±æ–‡å…³é”®è¯æœç´¢é€šé“ï¼‰
CN_TO_EN_QUERY = {
    "æ°”å€™å› å­": "climate",
    "ç‰©å€™": "phenology phenological",
    "ç‰©å€™å˜åŒ–": "phenology phenological",
    "è’¸æ•£å‘": "evapotranspiration transpiration",
    "è’¸æ•£": "evapotranspiration",
    "å¾„æµ": "runoff streamflow discharge",
    "æ°´æ–‡æ•ˆåº”": "hydrological hydrology streamflow runoff evapotranspiration",
    "æ°´æ–‡å“åº”": "hydrological response",
    "æ°´æ–‡": "hydrological hydrology",
    "æ°´å¾ªç¯": "water cycle hydrological",
    "æ°”å€™å˜åŒ–": "climate change warming",
    "ç”Ÿé•¿å­£": "growing season",
    "ç”Ÿé•¿å­£å»¶é•¿": "growing season lengthening",
    "è¿”é’æœŸ": "spring greenup",
    "è¿”é’": "greenup green-up",
    "æ¯é»„æœŸ": "autumn senescence",
    "æ¯é»„": "senescence",
    "æ¸©åº¦": "temperature warming",
    "é™æ°´": "precipitation rainfall",
    "æ¹¿åº¦": "humidity",
    "æ£®æ—": "forest",
    "æ¤è¢«": "vegetation",
    "å¹²æ—±": "drought",
    "å½’å› ": "attribution",
    "é¥æ„Ÿ": "remote sensing satellite NDVI",
    "ç¢³å¾ªç¯": "carbon cycle GPP",
    "æ°´æ–‡æ¨¡å‹": "hydrological model",
    "å½±å“": "effect impact",
    "æœºåˆ¶": "mechanism",
    "æ°”å€™é©±åŠ¨": "climate forcing",
    "åœŸå£¤æ°´åˆ†": "soil moisture",
    "å“åº”": "response",
    "è¶‹åŠ¿": "trend",
    "å˜æš–": "warming",
    "å¢åŠ ": "increase",
    "å‡å°‘": "decrease",
    "æ¨¡æ‹Ÿ": "simulation",
    "å† å±‚": "canopy",
    "å¶é¢ç§¯": "leaf area LAI",
    "ç¢³": "carbon GPP",
    "äº§æ°´é‡": "water yield",
    "ç§¯é›ª": "snow snowpack",
    "è‰åœ°": "grassland",
    "å†œç”°": "cropland",
    # æ¤è¢«åˆ†ç±»
    "å¤©ç„¶æ¤è¢«": "natural vegetation",
    "äººå·¥æ¤è¢«": "planted artificial vegetation",
    "å¤©ç„¶æ—": "natural forest",
    "äººå·¥æ—": "planted forest plantation",
    "é€ æ—": "afforestation reforestation",
    "é€€è€•è¿˜æ—": "afforestation reforestation",
    "åˆ’åˆ†": "classification mapping distinguish",
    "åˆ†ç±»": "classification",
    "åˆ¶å›¾": "mapping",
    "è¯†åˆ«": "identification detection",
    "æ°´æ–‡æ¨¡å‹": "hydrological model SWAT VIC",
    "ç”Ÿæ€æ°´æ–‡": "ecohydrology ecohydrological",
    # åœ°ç†åè¯
    "æµåŸŸ": "basin watershed catchment",
    "é«˜åŸ": "plateau",
    "é’è—é«˜åŸ": "Tibetan Plateau",
    "é»„åœŸé«˜åŸ": "Loess Plateau",
    "æµ·æ²³": "Haihe",
    "é»„æ²³": "Yellow River",
    "é•¿æ±Ÿ": "Yangtze",
    "ååŒ—": "North China",
    "ä¸œåŒ—": "Northeast China",
    "è¥¿åŒ—": "Northwest China",
}

# ============= è‹±â†’ä¸­æ ‡ç­¾æ˜ å°„ï¼ˆç»™è‹±æ–‡è®ºæ–‡ç”Ÿæˆä¸­æ–‡å…³é”®è¯ï¼‰ =============
_EN_TO_CN_TAGS = {
    # ç‰©å€™
    "phenology": "ç‰©å€™", "phenological": "ç‰©å€™", "phenophase": "ç‰©å€™",
    "growing season length": "ç”Ÿé•¿å­£é•¿åº¦", "growing season": "ç”Ÿé•¿å­£",
    "start of season": "è¿”é’æœŸ", "end of season": "æ¯é»„æœŸ",
    "green-up": "è¿”é’", "greenup": "è¿”é’", "budburst": "èŒèŠ½",
    "senescence": "æ¯é»„", "leaf-out": "å±•å¶", "leaf out": "å±•å¶",
    "spring onset": "æ˜¥å­£ç‰©å€™", "autumn": "ç§‹å­£",
    # è’¸æ•£å‘/æ°´æ–‡
    "evapotranspiration": "è’¸æ•£å‘", "transpiration": "è’¸è…¾",
    "potential evapotranspiration": "æ½œåœ¨è’¸æ•£å‘",
    "water cycle": "æ°´å¾ªç¯", "water balance": "æ°´é‡å¹³è¡¡",
    "water yield": "äº§æ°´é‡", "water resources": "æ°´èµ„æº",
    "hydrological response": "æ°´æ–‡å“åº”", "hydrological": "æ°´æ–‡", "hydrology": "æ°´æ–‡",
    "runoff": "å¾„æµ", "streamflow": "å¾„æµ", "discharge": "å¾„æµ",
    "baseflow": "åŸºæµ", "interception": "æˆªç•™",
    "snow": "ç§¯é›ª", "snowmelt": "èé›ª", "snowpack": "ç§¯é›ª",
    "flood": "æ´ªæ°´", "river": "æ²³æµ",
    # æ°”å€™
    "climate change": "æ°”å€™å˜åŒ–", "global warming": "å…¨çƒå˜æš–",
    "climate variability": "æ°”å€™å˜ç‡",
    "vapor pressure deficit": "é¥±å’Œæ°´æ±½å‹å·®VPD", "vpd": "é¥±å’Œæ°´æ±½å‹å·®VPD",
    "temperature": "æ¸©åº¦", "air temperature": "æ°”æ¸©",
    "precipitation": "é™æ°´", "rainfall": "é™æ°´",
    "humidity": "æ¹¿åº¦", "relative humidity": "æ¹¿åº¦",
    "solar radiation": "å¤ªé˜³è¾å°„", "radiation": "è¾å°„",
    "wind speed": "é£é€Ÿ", "wind": "é£",
    "warming": "å˜æš–", "frost": "éœœå†»", "co2": "CO2",
    # æ¤è¢«
    "vegetation": "æ¤è¢«", "forest": "æ£®æ—", "temperate forest": "æ¸©å¸¦æ£®æ—",
    "boreal": "åŒ—æ–¹é’ˆå¶æ—", "tropical": "çƒ­å¸¦", "alpine": "é«˜å¯’",
    "deciduous": "è½å¶", "conifer": "é’ˆå¶", "evergreen": "å¸¸ç»¿",
    "canopy": "å† å±‚", "leaf area index": "å¶é¢ç§¯æŒ‡æ•°", "lai": "å¶é¢ç§¯æŒ‡æ•°",
    "leaf area": "å¶é¢ç§¯", "leaf": "å¶ç‰‡",
    "grassland": "è‰åœ°", "cropland": "å†œç”°", "shrub": "çŒä¸›",
    # é¥æ„ŸæŒ‡æ•°
    "ndvi": "NDVI", "evi": "EVI", "sif": "SIF",
    "modis": "MODIS", "landsat": "Landsat", "remote sensing": "é¥æ„Ÿ",
    # ç¢³å¾ªç¯/ç”Ÿäº§åŠ›
    "carbon": "ç¢³", "carbon flux": "ç¢³é€šé‡",
    "gpp": "GPP", "gross primary": "GPP",
    "npp": "NPP", "net primary": "NPP",
    "photosynthesis": "å…‰åˆä½œç”¨", "respiration": "å‘¼å¸",
    "productivity": "ç”Ÿäº§åŠ›",
    "water use efficiency": "æ°´åˆ†åˆ©ç”¨æ•ˆç‡", "wue": "æ°´åˆ†åˆ©ç”¨æ•ˆç‡",
    "eddy covariance": "æ¶¡åº¦ç›¸å…³", "flux tower": "é€šé‡å¡”",
    # ç”Ÿæ€/åœ°ç†
    "ecosystem": "ç”Ÿæ€ç³»ç»Ÿ", "drought": "å¹²æ—±",
    "semi-arid": "åŠå¹²æ—±", "arid": "å¹²æ—±",
    "soil moisture": "åœŸå£¤æ°´åˆ†", "soil": "åœŸå£¤",
    "catchment": "æµåŸŸ", "watershed": "æµåŸŸ", "basin": "æµåŸŸ",
    "attribution": "å½’å› åˆ†æ", "trend": "è¶‹åŠ¿", "model": "æ¨¡å‹",
    "land use": "åœŸåœ°åˆ©ç”¨", "land cover": "åœŸåœ°è¦†ç›–",
    "elevation": "æµ·æ‹”", "latitude": "çº¬åº¦",
    "tibetan plateau": "é’è—é«˜åŸ", "china": "ä¸­å›½",
    "tree ring": "æ ‘è½®",
    # æ¤è¢«åˆ†ç±»
    "planted forest": "äººå·¥æ—", "plantation": "äººå·¥æ—", "planted": "äººå·¥",
    "natural forest": "å¤©ç„¶æ—", "natural vegetation": "å¤©ç„¶æ¤è¢«",
    "afforestation": "é€ æ—", "reforestation": "å†é€ æ—",
    "classification": "åˆ†ç±»", "mapping": "åˆ¶å›¾",
    # æ°´æ–‡æ¨¡å‹
    "swat": "SWATæ°´æ–‡æ¨¡å‹", "vic model": "VICæ¨¡å‹", "noah": "Noahæ¨¡å‹",
    "ecohydrolog": "ç”Ÿæ€æ°´æ–‡",
    "land surface model": "é™†é¢æ¨¡å‹", "process-based": "è¿‡ç¨‹æ¨¡å‹",
}
_COMPOUND_TAG_RULES = [
    # æ°”å€™-ç‰©å€™
    ({"æ°”å€™å˜åŒ–", "ç‰©å€™"}, "æ°”å€™å› å­ç‰©å€™å“åº”"),
    ({"æ¸©åº¦", "ç‰©å€™"}, "æ¸©åº¦é©±åŠ¨ç‰©å€™"),
    ({"å˜æš–", "ç‰©å€™"}, "æ¸©åº¦é©±åŠ¨ç‰©å€™"),
    ({"é™æ°´", "ç‰©å€™"}, "é™æ°´å½±å“ç‰©å€™"),
    ({"æ¹¿åº¦", "ç‰©å€™"}, "æ¹¿åº¦å½±å“ç‰©å€™"),
    ({"æ¸©åº¦", "æ¹¿åº¦", "ç‰©å€™"}, "å¤šæ°”å€™å› å­é©±åŠ¨ç‰©å€™"),
    ({"å¹²æ—±", "ç‰©å€™"}, "å¹²æ—±ç‰©å€™å“åº”"),
    ({"æ°”å€™å˜åŒ–", "ç”Ÿé•¿å­£"}, "æ°”å€™å˜åŒ–å½±å“ç”Ÿé•¿å­£"),
    ({"å˜æš–", "ç”Ÿé•¿å­£"}, "æ°”å€™å˜åŒ–å½±å“ç”Ÿé•¿å­£"),
    # ç‰©å€™-æ°´æ–‡ï¼ˆæ ¸å¿ƒäº¤å‰é¢†åŸŸï¼‰
    ({"ç‰©å€™", "è’¸æ•£å‘"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "å¾„æµ"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "æ°´å¾ªç¯"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "æ°´æ–‡"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "äº§æ°´é‡"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "æ°´æ–‡å“åº”"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç‰©å€™", "æ²³æµ"}, "ç‰©å€™å˜åŒ–æ°´æ–‡æ•ˆåº”"),
    ({"ç”Ÿé•¿å­£", "æ²³æµ"}, "ç”Ÿé•¿å­£å½±å“å¾„æµ"),
    ({"ç”Ÿé•¿å­£", "è’¸æ•£å‘"}, "ç”Ÿé•¿å­£å½±å“è’¸æ•£å‘"),
    ({"ç”Ÿé•¿å­£", "å¾„æµ"}, "ç”Ÿé•¿å­£å½±å“å¾„æµ"),
    ({"ç”Ÿé•¿å­£é•¿åº¦", "å¾„æµ"}, "ç”Ÿé•¿å­£å½±å“å¾„æµ"),
    ({"ç”Ÿé•¿å­£é•¿åº¦", "è’¸æ•£å‘"}, "ç”Ÿé•¿å­£å½±å“è’¸æ•£å‘"),
    # æ£®æ—/æ¤è¢«-æ°´æ–‡
    ({"æ£®æ—", "è’¸æ•£å‘"}, "æ£®æ—è’¸æ•£å‘"),
    ({"æ£®æ—", "å¾„æµ"}, "æ£®æ—æ°´æ–‡æ•ˆåº”"),
    ({"æ£®æ—", "æ°´æ–‡"}, "æ£®æ—æ°´æ–‡æ•ˆåº”"),
    ({"æ¤è¢«", "è’¸æ•£å‘"}, "æ¤è¢«è’¸æ•£å‘"),
    ({"æ¤è¢«", "å¾„æµ"}, "æ¤è¢«æ°´æ–‡æ•ˆåº”"),
    ({"æ¤è¢«", "æ°´æ–‡"}, "æ¤è¢«æ°´æ–‡æ•ˆåº”"),
    ({"æ¤è¢«", "ç‰©å€™"}, "æ¤è¢«ç‰©å€™"),
    # æ°”å€™-æ°´æ–‡
    ({"æ°”å€™å˜åŒ–", "è’¸æ•£å‘"}, "æ°”å€™å˜åŒ–è’¸æ•£å‘å“åº”"),
    ({"æ°”å€™å˜åŒ–", "å¾„æµ"}, "æ°”å€™å˜åŒ–å¾„æµå“åº”"),
    ({"æ°”å€™å˜åŒ–", "æ°´æ–‡"}, "æ°”å€™å˜åŒ–æ°´æ–‡å“åº”"),
    # ç¢³å¾ªç¯-ç‰©å€™
    ({"ç¢³", "ç‰©å€™"}, "ç‰©å€™ç¢³å¾ªç¯"),
    ({"GPP", "ç‰©å€™"}, "ç‰©å€™ç¢³å¾ªç¯"),
    ({"å…‰åˆä½œç”¨", "ç‰©å€™"}, "ç‰©å€™ç¢³å¾ªç¯"),
    ({"GPP", "ç”Ÿé•¿å­£"}, "ç‰©å€™ç¢³å¾ªç¯"),
    ({"ç”Ÿäº§åŠ›", "ç‰©å€™"}, "ç‰©å€™ç¢³å¾ªç¯"),
    # å¹²æ—±
    ({"å¹²æ—±", "æ¤è¢«"}, "å¹²æ—±æ¤è¢«å“åº”"),
    ({"å¹²æ—±", "è’¸æ•£å‘"}, "å¹²æ—±è’¸æ•£å‘å“åº”"),
    # æ°´åˆ†åˆ©ç”¨
    ({"æ°´åˆ†åˆ©ç”¨æ•ˆç‡", "æ¤è¢«"}, "æ¤è¢«æ°´åˆ†åˆ©ç”¨æ•ˆç‡"),
    ({"æ°´åˆ†åˆ©ç”¨æ•ˆç‡", "ç‰©å€™"}, "ç‰©å€™æ°´åˆ†åˆ©ç”¨æ•ˆç‡"),
    # ç”Ÿæ€æ°´æ–‡æ¨¡å‹
    ({"ç‰©å€™", "SWATæ°´æ–‡æ¨¡å‹"}, "ç‰©å€™æ°´æ–‡æ¨¡å‹"),
    ({"ç‰©å€™", "ç”Ÿæ€æ°´æ–‡"}, "ç‰©å€™æ°´æ–‡æ¨¡å‹"),
    ({"ç”Ÿé•¿å­£", "ç”Ÿæ€æ°´æ–‡"}, "ç‰©å€™æ°´æ–‡æ¨¡å‹"),
    # æ¤è¢«åˆ†ç±»
    ({"äººå·¥æ—", "å¤©ç„¶æ—"}, "å¤©ç„¶æ—äººå·¥æ—åˆ’åˆ†"),
    ({"äººå·¥", "å¤©ç„¶æ¤è¢«"}, "å¤©ç„¶æ—äººå·¥æ—åˆ’åˆ†"),
    ({"é€ æ—", "åˆ†ç±»"}, "å¤©ç„¶æ—äººå·¥æ—åˆ’åˆ†"),
    ({"é€ æ—", "åˆ¶å›¾"}, "å¤©ç„¶æ—äººå·¥æ—åˆ’åˆ†"),
]

def _generate_cn_topics(paper):
    """ä¸ºè‹±æ–‡è®ºæ–‡ç”Ÿæˆä¸­æ–‡ä¸»é¢˜æ ‡ç­¾"""
    parts = [paper.get('keywords', ''), paper.get('abstract', ''), paper.get('title_extracted', '')]
    text = ' '.join(p for p in parts if p).lower()
    # è‹¥æ ¸å¿ƒå…ƒæ•°æ®ä¸è¶³ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œè¡¥å……first_pages
    if len(text) < 100:
        fp = paper.get('first_pages', '')
        if fp:
            text += ' ' + fp[:2000].lower()
    cn = set()
    for en in sorted(_EN_TO_CN_TAGS.keys(), key=len, reverse=True):
        if en in text:
            cn.add(_EN_TO_CN_TAGS[en])
    # ä¹Ÿæ£€æŸ¥folderåç§°ä¸­çš„ä¸­æ–‡å…³é”®è¯
    folder = paper.get('folder', '')
    if folder:
        for kw in ['ç‰©å€™', 'æ°´æ–‡', 'è’¸æ•£å‘', 'å¾„æµ', 'æ°”å€™', 'ç”Ÿé•¿å­£', 'æ¤è¢«', 'æ£®æ—',
                    'å¹²æ—±', 'ç¢³', 'é¥æ„Ÿ', 'æ¨¡å‹', 'å½’å› ']:
            if kw in folder:
                cn.add(kw)
    for conds, tag in _COMPOUND_TAG_RULES:
        if conds.issubset(cn):
            cn.add(tag)
    return ' '.join(cn)

def _translate_query_wordlevel(query):
    """è¯çº§ç¿»è¯‘ï¼šä¸ç”¨æ¨¡æ¿ï¼Œä»…åšæœ€é•¿åŒ¹é…è¯ç¿»è¯‘ï¼ˆç”¨äºè‹±æ–‡å…³é”®è¯æœç´¢é€šé“ï¼‰"""
    q_norm = re.sub(r'[\sï¼Œã€‚ã€ï¼šï¼›ï¼Ÿï¼\u201c\u201d\u2018\u2019ï¼ˆï¼‰()çš„ä¸å’Œå¯¹åœ¨äºä¸­]+', '', query)
    total_cn_chars = len(re.findall(r'[\u4e00-\u9fff]', q_norm))
    text = q_norm
    parts = []
    seen = set()
    translated_chars = 0
    sorted_keys = sorted(CN_TO_EN_QUERY.keys(), key=len, reverse=True)
    while text:
        matched = False
        for key in sorted_keys:
            if text.startswith(key):
                en = CN_TO_EN_QUERY[key]
                if en not in seen:
                    parts.append(en)
                    seen.add(en)
                translated_chars += len(key)
                text = text[len(key):]
                matched = True
                break
        if not matched:
            m = re.match(r'[a-zA-Z]+', text)
            if m:
                w = m.group()
                if w not in seen:
                    parts.append(w)
                    seen.add(w)
                text = text[len(w):]
            else:
                text = text[1:]
    # è‹¥ç¿»è¯‘è¦†ç›–ç‡<50%ï¼ˆå¤§é‡ä¸­æ–‡åœ°å/ä¸“æœ‰åè¯æœªç¿»è¯‘ï¼‰ï¼Œè¿”å›ç©ºä»¥é¿å…æ³›åŒ–åŒ¹é…
    if total_cn_chars > 0 and translated_chars / total_cn_chars < 0.5:
        return ''
    return ' '.join(parts)

def _translate_query(query):
    """å°†ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘ä¸ºè‹±æ–‡ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰"""
    # 1) å…ˆå°è¯•å®Œæ•´æŸ¥è¯¢æ¨¡æ¿
    q_norm = re.sub(r'[\sï¼Œã€‚ã€ï¼šï¼›ï¼Ÿï¼\u201c\u201d\u2018\u2019ï¼ˆï¼‰()]+', '', query)
    for cn, en in _QUERY_TEMPLATES.items():
        if q_norm == re.sub(r'\s+', '', cn):
            return en
    for cn, en in sorted(_QUERY_TEMPLATES.items(), key=lambda x: len(x[0]), reverse=True):
        cn_norm = re.sub(r'\s+', '', cn)
        if q_norm in cn_norm:
            return en
    # 2) æœ€é•¿åŒ¹é…ä¼˜å…ˆï¼ˆè§£å†³jiebaåˆ‡è¯ä¸åŒ¹é…CN_TO_EN_QUERYé”®çš„é—®é¢˜ï¼‰
    text = q_norm
    parts = []
    seen = set()
    sorted_keys = sorted(CN_TO_EN_QUERY.keys(), key=len, reverse=True)
    while text:
        matched = False
        for key in sorted_keys:
            if text.startswith(key):
                en = CN_TO_EN_QUERY[key]
                if en not in seen:
                    parts.append(en)
                    seen.add(en)
                text = text[len(key):]
                matched = True
                break
        if not matched:
            # æ£€æŸ¥è‹±æ–‡å­—ç¬¦
            m = re.match(r'[a-zA-Z]+', text)
            if m:
                w = m.group()
                if w not in seen:
                    parts.append(w)
                    seen.add(w)
                text = text[len(w):]
            else:
                text = text[1:]  # è·³è¿‡æ— æ³•åŒ¹é…çš„å­—ç¬¦
    return ' '.join(parts)


# ============= v3: jiebaåˆ†è¯ =============
def tokenize(text):
    """åˆ†è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰- v3ä½¿ç”¨jiebaåˆ‡è¯"""
    tokens = set()

    # è‹±æ–‡å•è¯
    en_words = re.findall(r'[a-zA-Z][a-zA-Z0-9_-]{1,}', text)
    tokens.update(w.lower() for w in en_words if len(w) >= 2)

    # ä¸­æ–‡ï¼šjiebaæœç´¢æ¨¡å¼åˆ†è¯
    zh_text = ''.join(re.findall(r'[\u4e00-\u9fff]+', text))
    if zh_text:
        words = jieba.cut_for_search(zh_text)
        tokens.update(w for w in words if len(w) >= 2 and w not in STOPWORDS_ZH)

    return tokens


def parse_query(query):
    """è§£ææŸ¥è¯¢ï¼šä½¿ç”¨jiebaåˆ†è¯æå–ä¸­è‹±æ–‡æŸ¥è¯¢è¯"""
    terms = []

    # è‹±æ–‡å•è¯
    en_words = re.findall(r'[a-zA-Z][a-zA-Z0-9_-]+', query)
    terms.extend(en_words)

    # ä¸­æ–‡éƒ¨åˆ†ï¼šjiebaåˆ†è¯
    zh_parts = re.findall(r'[\u4e00-\u9fff]+', query)
    for zh in zh_parts:
        words = jieba.cut(zh)
        terms.extend(w for w in words if len(w) >= 2 and w not in STOPWORDS_ZH)

    return terms


def expand_query(query_terms):
    """æ‰©å±•æŸ¥è¯¢è¯ï¼ˆæ·»åŠ åŒä¹‰è¯/ç›¸å…³è¯ï¼‰"""
    expanded = set(t.lower() for t in query_terms)
    expanded.update(query_terms)  # ä¿ç•™åŸå§‹å¤§å°å†™
    matched_topics = []

    for term in query_terms:
        term_lower = term.lower()
        for topic, synonyms in TOPIC_EXPANSIONS.items():
            synonyms_lower = [s.lower() for s in synonyms]
            if term_lower in synonyms_lower or term in topic:
                expanded.update(s.lower() for s in synonyms)
                if topic not in matched_topics:
                    matched_topics.append(topic)

    return expanded, matched_topics


# ============= å…³é”®è¯æœç´¢ =============

def score_paper_keyword(paper, query_tokens, expanded_tokens):
    """è®¡ç®—è®ºæ–‡ä¸æŸ¥è¯¢çš„å…³é”®è¯ç›¸å…³æ€§å¾—åˆ†"""
    score = 0.0

    fields = {
        "filename": 3.0,
        "keywords": 5.0,
        "abstract": 4.0,
        "title_extracted": 3.5,
        "first_pages_text": 1.0,
        "folder": 2.0,
        "zotero_meta": 2.5,  # v3: Zoteroå…ƒæ•°æ®
        "cn_topics": 3.0,    # v4: è‹±æ–‡è®ºæ–‡çš„ä¸­æ–‡æ ‡ç­¾ï¼ˆé€‚åº¦æƒé‡ï¼Œé¿å…è¿‡åº¦å‹åˆ¶ä¸­æ–‡è®ºæ–‡ï¼‰
    }

    matched_fields = []
    matched_terms = set()
    precomputed = paper.get("tokens", {})

    for field, weight in fields.items():
        # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—tokens
        if field in precomputed:
            text_tokens = set(precomputed[field])
        elif field == "first_pages_text":
            # ä»…å¯¹æ— æ‘˜è¦æ— å…³é”®è¯çš„è®ºæ–‡æ£€ç´¢å…¨æ–‡
            if paper.get("abstract") or paper.get("keywords"):
                continue
            text = paper.get(field, "")
            if not text:
                continue
            text_tokens = tokenize(text)
        else:
            text = paper.get(field, "")
            if not text:
                continue
            text_tokens = tokenize(text)

        # ç²¾ç¡®åŒ¹é…ï¼ˆåŸå§‹æŸ¥è¯¢è¯ï¼‰
        exact_matches = query_tokens & text_tokens
        if exact_matches:
            score += len(exact_matches) * weight * 2.0
            matched_fields.append(field)
            matched_terms.update(exact_matches)

        # æ‰©å±•åŒ¹é…ï¼ˆåŒä¹‰è¯ï¼‰
        expanded_matches = (expanded_tokens - query_tokens) & text_tokens
        if expanded_matches:
            score += len(expanded_matches) * weight * 0.5
            matched_terms.update(expanded_matches)

    # åŠ åˆ†é¡¹
    if paper.get("abstract"):
        if "[å…œåº•æå–]" not in paper["abstract"]:
            score *= 1.2
        else:
            score *= 1.05
    if paper.get("keywords"):
        score *= 1.1

    # åŒ¹é…å­—æ®µå¤šæ ·æ€§åŠ åˆ†
    if len(matched_fields) >= 3:
        score *= 1.3

    # æŸ¥è¯¢æ¦‚å¿µè¦†ç›–ç‡åŠ åˆ†ï¼šä¼˜å…ˆè¿”å›åŒ¹é…äº†æ‰€æœ‰æŸ¥è¯¢æ¦‚å¿µçš„è®ºæ–‡
    if len(query_tokens) >= 2:
        coverage = len(matched_terms & query_tokens) / len(query_tokens)
        if coverage >= 0.9:
            score *= 2.0  # è¦†ç›–å‡ ä¹æ‰€æœ‰æŸ¥è¯¢è¯
        elif coverage >= 0.7:
            score *= 1.5  # è¦†ç›–å¤§éƒ¨åˆ†æŸ¥è¯¢è¯
        elif coverage >= 0.5:
            score *= 1.2  # è¦†ç›–åŠæ•°æŸ¥è¯¢è¯

    return score, matched_fields, matched_terms


def keyword_search(query, papers, top_n=50, folder_filter=None, exclude_fallback=False):
    """å…³é”®è¯æœç´¢"""
    query_terms = parse_query(query)
    query_tokens = set(t.lower() for t in query_terms)
    query_tokens.update(query_terms)

    expanded_tokens, matched_topics = expand_query(query_terms)

    results = []
    for paper in papers:
        if folder_filter and folder_filter not in paper.get("folder", ""):
            continue
        if paper.get("is_scannable"):
            continue
        fname = paper.get("filename", "")
        if "å‘æ˜ä¸“åˆ©" in fname or "ä¸“è‘—" in fname:
            continue
        if exclude_fallback and paper.get("abstract", "").startswith("[å…œåº•æå–]"):
            continue

        s, matched, terms = score_paper_keyword(paper, query_tokens, expanded_tokens)
        if s > 0:
            results.append((s, matched, terms, paper))

    results.sort(key=lambda x: x[0], reverse=True)

    # å»é‡ï¼ˆå«æ¨¡ç³Šå»é‡ï¼šå»æ‰"è®ºæ–‡53-"ç­‰ç¼–å·å‰ç¼€ï¼‰
    def _norm_fn(name):
        return re.sub(r'^(?:è®ºæ–‡)?\d+[\.\-\s]+', '', name).replace(' ', '')

    seen = set()
    seen_norm = set()
    deduped = []
    for item in results:
        fn = item[3]["filename"]
        norm = _norm_fn(fn)
        if fn in seen or norm in seen_norm:
            continue
        seen.add(fn)
        seen_norm.add(norm)
        deduped.append(item)

    return deduped[:top_n], matched_topics


# ============= è¯­ä¹‰æœç´¢ =============

_embeddings_cache = {}

def load_embeddings():
    """åŠ è½½embeddingç´¢å¼•ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if 'data' in _embeddings_cache:
        return _embeddings_cache['data']

    if not EMBEDDINGS_PATH.exists():
        return None

    data = np.load(EMBEDDINGS_PATH, allow_pickle=True)
    result = {
        'embeddings': data['embeddings'],
        'filenames': list(data['filenames']),
        'model_name': str(data.get('model_name', 'unknown')),
    }
    # å»ºç«‹filenameâ†’indexæ˜ å°„
    result['filename_to_idx'] = {fn: i for i, fn in enumerate(result['filenames'])}
    _embeddings_cache['data'] = result
    return result


_model_cache = {}

def get_embedding_model(model_name):
    """åŠ è½½embeddingæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if model_name in _model_cache:
        return _model_cache[model_name]

    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer(model_name)
    _model_cache[model_name] = model
    return model


def semantic_search(query, papers, top_n=50, folder_filter=None):
    """è¯­ä¹‰æœç´¢ï¼šä¸­è‹±æ–‡å„è·‘ä¸€è½®ï¼ŒRRFåˆå¹¶æ’å"""
    emb_data = load_embeddings()
    if emb_data is None:
        return []

    model = get_embedding_model(emb_data['model_name'])

    # åŒè¯­æŸ¥è¯¢
    en_query = _translate_query(query)
    has_en = bool(en_query.strip()) and en_query != query

    queries = [query]
    if has_en:
        queries.append(en_query)

    query_embeddings = model.encode(queries, normalize_embeddings=True)

    # æ„å»ºæ–‡ä»¶åâ†’è®ºæ–‡çš„æ˜ å°„å’Œè¿‡æ»¤é›†
    fn_to_paper = {}
    skip_fns = set()
    for p in papers:
        fn = p.get('filename', '')
        fn_to_paper[fn] = p
        if p.get("is_scannable"):
            skip_fns.add(fn)
        if "å‘æ˜ä¸“åˆ©" in fn or "ä¸“è‘—" in fn:
            skip_fns.add(fn)
        if folder_filter and folder_filter not in p.get("folder", ""):
            skip_fns.add(fn)

    # å¯¹æ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆæ’å
    all_rankings = []  # list of {filename: rank}
    for qe in query_embeddings:
        sims = emb_data['embeddings'] @ qe
        ranking = {}
        rank = 0
        for idx in np.argsort(sims)[::-1]:
            fn = emb_data['filenames'][idx]
            if fn in skip_fns or fn not in fn_to_paper:
                continue
            if sims[idx] < 0.1:
                break
            rank += 1
            ranking[fn] = rank
        all_rankings.append(ranking)

    # RRFåˆå¹¶æ’åï¼ˆk=30ï¼‰
    k = 30
    rrf_scores = {}
    all_fns = set()
    for ranking in all_rankings:
        all_fns.update(ranking.keys())
    for fn in all_fns:
        score = 0.0
        for ranking in all_rankings:
            if fn in ranking:
                score += 1.0 / (k + ranking[fn])
        rrf_scores[fn] = score

    # æŒ‰RRFåˆ†æ•°æ’åºï¼Œè¾“å‡º (sim, paper) æ ¼å¼ï¼ˆsimç”¨ä¸­æ–‡æŸ¥è¯¢çš„å€¼ä¾›æ˜¾ç¤ºï¼‰
    sims_cn = emb_data['embeddings'] @ query_embeddings[0]
    fn_to_emb_idx = {fn: i for i, fn in enumerate(emb_data['filenames'])}

    sorted_fns = sorted(rrf_scores.keys(), key=lambda fn: rrf_scores[fn], reverse=True)
    results = []
    for fn in sorted_fns:
        paper = fn_to_paper.get(fn)
        if paper is None:
            continue
        emb_idx = fn_to_emb_idx.get(fn)
        sim = float(sims_cn[emb_idx]) if emb_idx is not None else 0.0
        results.append((sim, paper))
        if len(results) >= top_n:
            break

    return results


# ============= æ··åˆæœç´¢ (RRF) =============

def _is_chinese_query(query):
    """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    return bool(re.search(r'[\u4e00-\u9fff]', query))

def hybrid_search(query, papers, top_n=10, folder_filter=None, exclude_fallback=False, extra_queries=None):
    """æ··åˆæœç´¢ï¼šå…³é”®è¯ + è¯­ä¹‰ + è·¨è¯­è¨€å…³é”®è¯ï¼Œä½¿ç”¨Reciprocal Rank Fusion (RRF)

    extra_queries: é¢å¤–æŸ¥è¯¢åˆ—è¡¨ï¼ˆå¦‚è‹±æ–‡ç¿»è¯‘ï¼‰ï¼Œæ¯ä¸ªæŸ¥è¯¢ç‹¬ç«‹èµ°å…³é”®è¯+è¯­ä¹‰é€šé“ï¼Œä¸ä¸»æŸ¥è¯¢RRFèåˆã€‚
                   è¿™æ ·Claudeå¯ä»¥ç›´æ¥ä¼ å…¥ç¿»è¯‘å¥½çš„è‹±æ–‡æŸ¥è¯¢ï¼Œæ— éœ€ä¾èµ–å†…ç½®è¯å…¸ã€‚
    """
    # é€šé“1: ä¸»æŸ¥è¯¢å…³é”®è¯æœç´¢
    kw_results, matched_topics = keyword_search(
        query, papers, top_n=200, folder_filter=folder_filter, exclude_fallback=exclude_fallback
    )

    # é€šé“2: ä¸»æŸ¥è¯¢è¯­ä¹‰æœç´¢ï¼ˆå·²å†…ç½®CN/ENåŒé€šé“RRFï¼‰
    sem_results = semantic_search(query, papers, top_n=200, folder_filter=folder_filter)

    # é€šé“3: è·¨è¯­è¨€å…³é”®è¯æœç´¢ï¼ˆä¸­æ–‡æŸ¥è¯¢â†’è‹±æ–‡è¯çº§ç¿»è¯‘ï¼Œæœç´¢è‹±æ–‡è®ºæ–‡åŸå§‹å­—æ®µï¼‰
    en_kw_results = []
    if _is_chinese_query(query):
        en_query = _translate_query_wordlevel(query)
        if en_query and en_query.strip():
            en_kw_results, _ = keyword_search(
                en_query, papers, top_n=200, folder_filter=folder_filter, exclude_fallback=exclude_fallback
            )

    # RRFèåˆ
    k = 60  # RRFå¸¸æ•°
    paper_scores = defaultdict(float)
    paper_data = {}  # filename â†’ (matched_fields, matched_terms, paper)

    # é€šé“1: ä¸»æŸ¥è¯¢å…³é”®è¯æ’åè´¡çŒ®
    for rank, (score, matched, terms, paper) in enumerate(kw_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k + rank + 1)
        paper_data[fn] = (matched, terms, paper, score)

    # é€šé“2: ä¸»æŸ¥è¯¢è¯­ä¹‰æ’åè´¡çŒ®
    for rank, (sim, paper) in enumerate(sem_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k + rank + 1)
        if fn not in paper_data:
            paper_data[fn] = (["semantic"], set(), paper, 0)

    # é€šé“3: å†…ç½®ç¿»è¯‘è‹±æ–‡å…³é”®è¯ï¼ˆk=100ï¼Œä½œä¸ºè½»å¾®æå‡ï¼‰
    k_en = 100
    for rank, (score, matched, terms, paper) in enumerate(en_kw_results):
        fn = paper["filename"]
        paper_scores[fn] += 1.0 / (k_en + rank + 1)
        if fn not in paper_data:
            paper_data[fn] = (matched, terms, paper, score)

    # é¢å¤–æŸ¥è¯¢é€šé“ï¼ˆç”±è°ƒç”¨æ–¹æä¾›ï¼Œå¦‚Claudeç¿»è¯‘çš„è‹±æ–‡æŸ¥è¯¢ï¼‰
    extra_sem_results_all = []
    if extra_queries:
        for eq in extra_queries:
            eq = eq.strip()
            if not eq:
                continue
            # é¢å¤–æŸ¥è¯¢çš„å…³é”®è¯é€šé“
            eq_kw, _ = keyword_search(eq, papers, top_n=200, folder_filter=folder_filter,
                                       exclude_fallback=exclude_fallback)
            for rank, (score, matched, terms, paper) in enumerate(eq_kw):
                fn = paper["filename"]
                paper_scores[fn] += 1.0 / (k + rank + 1)
                if fn not in paper_data:
                    paper_data[fn] = (matched, terms, paper, score)

            # é¢å¤–æŸ¥è¯¢çš„è¯­ä¹‰é€šé“
            eq_sem = semantic_search(eq, papers, top_n=200, folder_filter=folder_filter)
            extra_sem_results_all.extend(eq_sem)
            for rank, (sim, paper) in enumerate(eq_sem):
                fn = paper["filename"]
                paper_scores[fn] += 1.0 / (k + rank + 1)
                if fn not in paper_data:
                    paper_data[fn] = (["semantic"], set(), paper, 0)

    # æŒ‰RRFåˆ†æ•°æ’åº
    sorted_fns = sorted(paper_scores.keys(), key=lambda fn: paper_scores[fn], reverse=True)

    results = []
    all_sem = sem_results + extra_sem_results_all
    for fn in sorted_fns[:top_n]:
        matched, terms, paper, kw_score = paper_data[fn]
        rrf_score = paper_scores[fn]

        # è·å–è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå–æ‰€æœ‰è¯­ä¹‰é€šé“çš„æœ€é«˜å€¼ï¼‰
        sem_sim = 0.0
        for sim, p in all_sem:
            if p["filename"] == fn:
                sem_sim = max(sem_sim, sim)

        # åœ¨ä¸»æŸ¥è¯¢å…³é”®è¯æœç´¢ä¸­çš„æ’å
        kw_rank = -1
        for i, (_, _, _, p) in enumerate(kw_results):
            if p["filename"] == fn:
                kw_rank = i + 1
                break

        # åœ¨ä¸»æŸ¥è¯¢è¯­ä¹‰æœç´¢ä¸­çš„æ’å
        sem_rank = -1
        for i, (_, p) in enumerate(sem_results):
            if p["filename"] == fn:
                sem_rank = i + 1
                break

        results.append({
            "paper": paper,
            "rrf_score": rrf_score,
            "kw_score": kw_score,
            "sem_sim": sem_sim,
            "kw_rank": kw_rank,
            "sem_rank": sem_rank,
            "matched_fields": matched,
            "matched_terms": terms,
        })

    return results, matched_topics


# ============= ç›¸ä¼¼è®ºæ–‡æ¨è =============

def find_similar(query_name, papers, top_n=10):
    """ç›¸ä¼¼è®ºæ–‡æ¨è"""
    target = None
    for p in papers:
        if query_name.lower() in p["filename"].lower():
            target = p
            break
    if not target:
        return [], [], query_name

    # æ„å»ºæœç´¢æŸ¥è¯¢
    search_text = f"{target.get('keywords', '')} {target.get('abstract', '')[:200]}"
    if not search_text.strip():
        search_text = target.get("first_pages_text", "")[:500]

    # ç”¨æ··åˆæœç´¢
    results, topics = hybrid_search(search_text, papers, top_n=top_n + 1)

    # æ’é™¤è‡ªèº«
    results = [r for r in results if r["paper"]["filename"] != target["filename"]][:top_n]

    return results, topics, target["filename"]


# ============= æ ¼å¼åŒ–è¾“å‡º =============

def format_results(results, query, matched_topics=None, similar_source=None, search_mode="hybrid"):
    """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
    lines = []

    if similar_source:
        lines.append(f"## ä¸ \"{similar_source}\" ç›¸ä¼¼çš„è®ºæ–‡")
    else:
        lines.append(f"## æœç´¢: \"{query}\" [{search_mode}]")

    if matched_topics:
        lines.append(f"æ‰©å±•ä¸»é¢˜: {', '.join(matched_topics)}")

    lines.append(f"æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡\n")

    for rank, item in enumerate(results, 1):
        if isinstance(item, dict):
            # æ··åˆæœç´¢ç»“æœ
            p = item["paper"]
            rrf = item["rrf_score"]
            kw_score = item["kw_score"]
            sem_sim = item["sem_sim"]
            kw_rank = item["kw_rank"]
            sem_rank = item["sem_rank"]
            matched = item["matched_fields"]
            terms = item["matched_terms"]

            score_parts = []
            if kw_rank > 0:
                score_parts.append(f"å…³é”®è¯#{kw_rank}")
            if sem_rank > 0:
                score_parts.append(f"è¯­ä¹‰#{sem_rank}({sem_sim:.2f})")
            score_info = " | ".join(score_parts) if score_parts else ""

            lines.append(f"### [{rank}] RRF: {rrf:.4f}  {score_info}")
        else:
            # æ—§æ ¼å¼å…¼å®¹
            score, matched, terms, p = item
            lines.append(f"### [{rank}] ç›¸å…³åº¦: {score:.1f}")

        lang = "ä¸­" if p["language"] == "zh" else "è‹±"
        year = p["year"] or "?"
        pages = p["page_count"]
        thesis = " ğŸ“" if p.get("is_thesis") else ""
        source = " ğŸ“šZ" if p.get("source") == "zotero" else ""

        lines.append(f"**{p['filename']}** ({lang}, {year}, {pages}é¡µ{thesis}{source})")
        lines.append(f"æ–‡ä»¶å¤¹: {p['folder']}")

        if matched:
            lines.append(f"åŒ¹é…å­—æ®µ: {', '.join(matched)}")
        if terms:
            display_terms = sorted(terms, key=len, reverse=True)[:10]
            lines.append(f"åŒ¹é…è¯: {', '.join(display_terms)}")

        # æ˜¾ç¤ºZoteroå…ƒæ•°æ®
        if p.get("zotero_title"):
            lines.append(f"Zoteroæ ‡é¢˜: {p['zotero_title']}")
        if p.get("zotero_authors"):
            authors = ", ".join(p["zotero_authors"][:3])
            if len(p["zotero_authors"]) > 3:
                authors += f" ç­‰({len(p['zotero_authors'])}äºº)"
            lines.append(f"ä½œè€…: {authors}")

        if p["keywords"]:
            lines.append(f"å…³é”®è¯: {p['keywords'][:300]}")
        if p["abstract"]:
            abs_text = p["abstract"][:500]
            if len(p["abstract"]) > 500:
                abs_text += "..."
            lines.append(f"æ‘˜è¦: {abs_text}")

        lines.append(f"è·¯å¾„: {p['path']}")
        lines.append("")

    return "\n".join(lines)


def show_stats():
    """æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡"""
    index = load_index()
    stats = index["stats"]

    print("=== æ–‡çŒ®ç´¢å¼•ç»Ÿè®¡ v3 ===")
    for k, v in stats.items():
        if k == "top_keywords":
            continue
        if k == "by_method":
            print(f"  æå–æ–¹æ³•åˆ†å¸ƒ:")
            for mk, mv in v.items():
                print(f"    {mk}: {mv}")
        else:
            print(f"  {k}: {v}")

    # æŒ‰æ–‡ä»¶å¤¹ç»Ÿè®¡
    by_folder = defaultdict(lambda: {"total": 0, "with_abs": 0, "thesis": 0, "local": 0, "zotero": 0})
    for p in index["papers"]:
        f = p["folder"]
        by_folder[f]["total"] += 1
        if p["abstract"]:
            by_folder[f]["with_abs"] += 1
        if p.get("is_thesis"):
            by_folder[f]["thesis"] += 1
        if p.get("source") == "zotero":
            by_folder[f]["zotero"] += 1
        else:
            by_folder[f]["local"] += 1

    print("\n=== æŒ‰æ–‡ä»¶å¤¹/åˆ†ç±»åˆ†å¸ƒ ===")
    for folder in sorted(by_folder.keys()):
        d = by_folder[folder]
        src = f"æœ¬åœ°{d['local']}" + (f"+Z{d['zotero']}" if d['zotero'] else "")
        print(f"  {folder}: {d['total']}ç¯‡ ({src}, æ‘˜è¦{d['with_abs']})")

    # EmbeddingçŠ¶æ€
    if EMBEDDINGS_PATH.exists():
        data = np.load(EMBEDDINGS_PATH, allow_pickle=True)
        print(f"\n=== Embeddingç´¢å¼• ===")
        print(f"  è®ºæ–‡æ•°: {len(data['filenames'])}")
        print(f"  å‘é‡ç»´åº¦: {data['embeddings'].shape[1]}")
        print(f"  æ¨¡å‹: {data.get('model_name', 'unknown')}")
        print(f"  æ–‡ä»¶å¤§å°: {EMBEDDINGS_PATH.stat().st_size/1024/1024:.1f} MB")
    else:
        print(f"\nâš ï¸ æ— Embeddingç´¢å¼• (è¿è¡Œ python3 build_embeddings.py åˆ›å»º)")

    # é«˜é¢‘å…³é”®è¯
    top_kw = stats.get("top_keywords", [])
    if top_kw:
        print(f"\n=== Top 30 é«˜é¢‘å…³é”®è¯ ===")
        for item in top_kw[:30]:
            print(f"  {item['keyword']}: {item['count']}")


def load_index():
    with open(INDEX_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    # ä¸ºè‹±æ–‡è®ºæ–‡åŠ¨æ€ç”Ÿæˆcn_topicså­—æ®µ
    for p in data["papers"]:
        if p.get("language") == "en" and not p.get("cn_topics"):
            p["cn_topics"] = _generate_cn_topics(p)
    return data


def main():
    args = sys.argv[1:]

    if not args:
        print(__doc__)
        return

    if "--stats" in args:
        show_stats()
        return

    top_n = 10
    folder_filter = None
    topic_mode = False
    year_sort = False
    similar_mode = False
    exclude_fallback = False
    search_mode = "hybrid"  # hybrid, keyword, semantic
    query_parts = []
    also_queries = []  # é¢å¤–æŸ¥è¯¢ï¼ˆå¤šæŸ¥è¯¢RRFèåˆï¼‰

    i = 0
    while i < len(args):
        if args[i] == "--top" and i + 1 < len(args):
            top_n = int(args[i + 1])
            i += 2
        elif args[i] == "--folder" and i + 1 < len(args):
            folder_filter = args[i + 1]
            i += 2
        elif args[i] == "--topic":
            topic_mode = True
            i += 1
        elif args[i] == "--year-sort":
            year_sort = True
            i += 1
        elif args[i] == "--similar":
            similar_mode = True
            i += 1
        elif args[i] == "--no-fallback":
            exclude_fallback = True
            i += 1
        elif args[i] == "--keyword":
            search_mode = "keyword"
            i += 1
        elif args[i] == "--semantic":
            search_mode = "semantic"
            i += 1
        elif args[i] == "--hybrid":
            search_mode = "hybrid"
            i += 1
        elif args[i] == "--also" and i + 1 < len(args):
            also_queries.append(args[i + 1])
            i += 2
        else:
            query_parts.append(args[i])
            i += 1

    query = " ".join(query_parts)
    if not query:
        print("è¯·æä¾›æœç´¢å…³é”®è¯")
        return

    # åŠ è½½ç´¢å¼•
    index = load_index()
    papers = index["papers"]

    if similar_mode:
        results, topics, source = find_similar(query, papers, top_n=top_n)
        output = format_results(results, query, matched_topics=topics, similar_source=source)
        print(output)
        return

    # æ ¹æ®æ¨¡å¼æœç´¢
    if search_mode == "keyword":
        results, topics = keyword_search(query, papers, top_n=top_n,
                                         folder_filter=folder_filter,
                                         exclude_fallback=exclude_fallback)
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted = []
        for score, matched, terms, paper in results:
            formatted.append({
                "paper": paper,
                "rrf_score": score,
                "kw_score": score,
                "sem_sim": 0,
                "kw_rank": 0,
                "sem_rank": 0,
                "matched_fields": matched,
                "matched_terms": terms,
            })
        output = format_results(formatted, query, matched_topics=topics, search_mode="keyword")

    elif search_mode == "semantic":
        if not EMBEDDINGS_PATH.exists():
            print("âš ï¸ æ— Embeddingç´¢å¼•ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢")
            print("  è¯·å…ˆè¿è¡Œ: python3 build_embeddings.py")
            search_mode = "keyword"
            results, topics = keyword_search(query, papers, top_n=top_n,
                                             folder_filter=folder_filter)
            formatted = []
            for score, matched, terms, paper in results:
                formatted.append({
                    "paper": paper,
                    "rrf_score": score,
                    "kw_score": score,
                    "sem_sim": 0,
                    "kw_rank": 0,
                    "sem_rank": 0,
                    "matched_fields": matched,
                    "matched_terms": terms,
                })
            output = format_results(formatted, query, matched_topics=topics, search_mode="keyword(fallback)")
        else:
            sem_results = semantic_search(query, papers, top_n=top_n, folder_filter=folder_filter)
            formatted = []
            for rank, (sim, paper) in enumerate(sem_results):
                formatted.append({
                    "paper": paper,
                    "rrf_score": sim,
                    "kw_score": 0,
                    "sem_sim": sim,
                    "kw_rank": 0,
                    "sem_rank": rank + 1,
                    "matched_fields": ["semantic"],
                    "matched_terms": set(),
                })
            output = format_results(formatted, query, search_mode="semantic")

    else:  # hybrid
        if not EMBEDDINGS_PATH.exists():
            # æ— embeddingï¼Œå›é€€åˆ°çº¯å…³é”®è¯æœç´¢
            results, topics = keyword_search(query, papers, top_n=top_n,
                                             folder_filter=folder_filter,
                                             exclude_fallback=exclude_fallback)
            formatted = []
            for score, matched, terms, paper in results:
                formatted.append({
                    "paper": paper,
                    "rrf_score": score,
                    "kw_score": score,
                    "sem_sim": 0,
                    "kw_rank": 0,
                    "sem_rank": 0,
                    "matched_fields": matched,
                    "matched_terms": terms,
                })
            output = format_results(formatted, query, matched_topics=topics, search_mode="keyword(no embedding)")
        else:
            results, topics = hybrid_search(query, papers, top_n=top_n,
                                            folder_filter=folder_filter,
                                            exclude_fallback=exclude_fallback,
                                            extra_queries=also_queries if also_queries else None)
            mode_label = f"hybrid+{len(also_queries)}q" if also_queries else "hybrid"
            output = format_results(results, query, matched_topics=topics, search_mode=mode_label)

    if year_sort and search_mode != "semantic":
        # å¹´ä»½æ’åºæ¨¡å¼ä¸‹é‡æ’
        print("(ç»“æœå·²æŒ‰ç›¸å…³åº¦æ’åºï¼Œæ·»åŠ --year-sortä»…åœ¨keywordæ¨¡å¼ä¸‹æŒ‰å¹´ä»½æ’åº)")

    print(output)


if __name__ == "__main__":
    main()
